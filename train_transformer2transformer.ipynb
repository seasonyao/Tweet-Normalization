{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pre-pocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tweetNormalizer import normalizeTweet\n",
    "import re\n",
    "\n",
    "# load train and validation data\n",
    "train_dataset = pd.read_json(\"WNUT2015_dataset/train_data.json\", orient=\"records\")\n",
    "val_dataset = pd.read_json(\"WNUT2015_dataset/test_truth.json\", orient=\"records\")\n",
    "\n",
    "train_dataset = train_dataset[:100]\n",
    "val_dataset = val_dataset[:5]\n",
    "\n",
    "\n",
    "make_sentence = lambda x : normalizeTweet(\" \".join(x))\n",
    "\n",
    "train_dataset['input_sentence'] = train_dataset['input'].apply(make_sentence)\n",
    "train_dataset['output_sentence'] = train_dataset['output'].apply(make_sentence)\n",
    "val_dataset['input_sentence'] = val_dataset['input'].apply(make_sentence)\n",
    "val_dataset['output_sentence'] = val_dataset['output'].apply(make_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> RT @USER : [ HQ ] 14@@ 05@@ 26 Xi@@ umin , Luhan @USER MBC Idol Fut@@ sal Championship ( cr . shade of the bloom ) HTTPURL </s>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertweetTokenizer, AutoTokenizer\n",
    "from tweetNormalizer import normalizeTweet\n",
    "\n",
    "#line = \"SC has first two presumptive cases of coronavirus, DHEC confirms https://postandcourier.com/health/covid19/sc-has-first-two-presumptive-cases-of-coronavirus-dhec-confirms/article_bddfe4ae-5fd3-11ea-9ce4-5f495366cee6.html?utm_medium=social&utm_source=twitter&utm_campaign=user-share… via @postandcourier\"\n",
    "line = \"RT @EXOffical : [ HQ ] 140526 Xiumin , Luhan @ MBC Idol Futsal Championship ( cr . shade of the bloom ) http://t.co/ToBKl76SzP\"\n",
    "\n",
    "\n",
    "tokenizer = BertweetTokenizer.from_pretrained(\"vinai/bertweet-base\")\n",
    "input_ids = torch.tensor([tokenizer.encode(normalizeTweet(line))])\n",
    "print(' '.join(tokenizer.convert_ids_to_tokens(input_ids[0], skip_special_tokens=False)))\n",
    "\n",
    "# tokenizer = BertweetTokenizer.from_pretrained(\"vinai/bertweet-base\", normalization=True)\n",
    "# input_ids = torch.tensor([tokenizer.encode(line)])\n",
    "# print(' '.join(tokenizer.convert_ids_to_tokens(input_ids[0], skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64000\n",
      "0 64000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bos_token': '<s>',\n",
       " 'eos_token': '</s>',\n",
       " 'unk_token': '<unk>',\n",
       " 'sep_token': '</s>',\n",
       " 'pad_token': '<pad>',\n",
       " 'cls_token': '<s>',\n",
       " 'mask_token': '<mask>',\n",
       " 'additional_special_tokens': \"['@USER', 'HTTPURL']\"}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", normalization=True)\n",
    "special_tokens_dict = {'additional_special_tokens': ['@USER','HTTPURL']}\n",
    "print(tokenizer.vocab_size)\n",
    "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "print(num_added_toks, tokenizer.vocab_size)\n",
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30522\n",
      "2 30522\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'unk_token': '[UNK]',\n",
       " 'sep_token': '[SEP]',\n",
       " 'pad_token': '[PAD]',\n",
       " 'cls_token': '[CLS]',\n",
       " 'mask_token': '[MASK]',\n",
       " 'additional_special_tokens': \"['@USER', 'HTTPURL']\"}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "special_tokens_dict = {'additional_special_tokens': ['@USER','HTTPURL']}\n",
    "print(tokenizer.vocab_size)\n",
    "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "print(num_added_toks, tokenizer.vocab_size)\n",
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@USER', 'HTTPURL']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.additional_special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50257\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bos_token': '<|endoftext|>',\n",
       " 'eos_token': '<|endoftext|>',\n",
       " 'unk_token': '<|endoftext|>',\n",
       " 'additional_special_tokens': \"['@USER', 'HTTPURL']\"}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "special_tokens_dict = {'additional_special_tokens': ['@USER','HTTPURL']}\n",
    "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "print(tokenizer.vocab_size)\n",
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50265\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bos_token': '<s>',\n",
       " 'eos_token': '</s>',\n",
       " 'unk_token': '<unk>',\n",
       " 'sep_token': '</s>',\n",
       " 'pad_token': '<pad>',\n",
       " 'cls_token': '<s>',\n",
       " 'mask_token': '<mask>',\n",
       " 'additional_special_tokens': \"['@USER', 'HTTPURL']\"}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "special_tokens_dict = {'additional_special_tokens': ['@USER','HTTPURL']}\n",
    "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "print(tokenizer.vocab_size)\n",
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sc has first two presumptive cases of coronavirus, dhec confirms HTTPURL... via @USER'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line = \"SC has first two presumptive cases of coronavirus, DHEC confirms https://postandcourier.com/health/covid19/sc-has-first-two-presumptive-cases-of-coronavirus-dhec-confirms/article_bddfe4ae-5fd3-11ea-9ce4-5f495366cee6.html?utm_medium=social&utm_source=twitter&utm_campaign=user-share… via @postandcourier\"\n",
    "\n",
    "ENCODER = \"bert-base-uncased\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(ENCODER)\n",
    "\n",
    "if ENCODER==\"bert-base-uncased\":\n",
    "    # CLS token will work as BOS token, SEP token will work as EOS token\n",
    "    tokenizer.bos_token = tokenizer.cls_token\n",
    "    tokenizer.eos_token = tokenizer.sep_token\n",
    "\n",
    "special_tokens_dict = {'additional_special_tokens': ['@USER','HTTPURL']}\n",
    "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "input_ids = tokenizer.encode(normalizeTweet(line))\n",
    "\n",
    "for rm_id in [tokenizer.bos_token_id, tokenizer.eos_token_id, tokenizer.pad_token_id]:\n",
    "    if rm_id in input_ids:\n",
    "        input_ids.remove(rm_id)\n",
    "\n",
    "input_ids = torch.tensor([input_ids])\n",
    "\n",
    "input_sent = tokenizer.decode(input_ids[0], skip_special_tokens=False)\n",
    "input_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertLMHeadModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertLMHeadModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.self.query.bias', 'bert.encoder.layer.1.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.self.value.bias', 'bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.3.crossattention.self.query.weight', 'bert.encoder.layer.3.crossattention.self.query.bias', 'bert.encoder.layer.3.crossattention.self.key.weight', 'bert.encoder.layer.3.crossattention.self.key.bias', 'bert.encoder.layer.3.crossattention.self.value.weight', 'bert.encoder.layer.3.crossattention.self.value.bias', 'bert.encoder.layer.3.crossattention.output.dense.weight', 'bert.encoder.layer.3.crossattention.output.dense.bias', 'bert.encoder.layer.3.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.3.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.5.crossattention.self.query.weight', 'bert.encoder.layer.5.crossattention.self.query.bias', 'bert.encoder.layer.5.crossattention.self.key.weight', 'bert.encoder.layer.5.crossattention.self.key.bias', 'bert.encoder.layer.5.crossattention.self.value.weight', 'bert.encoder.layer.5.crossattention.self.value.bias', 'bert.encoder.layer.5.crossattention.output.dense.weight', 'bert.encoder.layer.5.crossattention.output.dense.bias', 'bert.encoder.layer.5.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.5.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.7.crossattention.self.query.weight', 'bert.encoder.layer.7.crossattention.self.query.bias', 'bert.encoder.layer.7.crossattention.self.key.weight', 'bert.encoder.layer.7.crossattention.self.key.bias', 'bert.encoder.layer.7.crossattention.self.value.weight', 'bert.encoder.layer.7.crossattention.self.value.bias', 'bert.encoder.layer.7.crossattention.output.dense.weight', 'bert.encoder.layer.7.crossattention.output.dense.bias', 'bert.encoder.layer.7.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.7.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.9.crossattention.self.query.weight', 'bert.encoder.layer.9.crossattention.self.query.bias', 'bert.encoder.layer.9.crossattention.self.key.weight', 'bert.encoder.layer.9.crossattention.self.key.bias', 'bert.encoder.layer.9.crossattention.self.value.weight', 'bert.encoder.layer.9.crossattention.self.value.bias', 'bert.encoder.layer.9.crossattention.output.dense.weight', 'bert.encoder.layer.9.crossattention.output.dense.bias', 'bert.encoder.layer.9.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.9.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.11.crossattention.self.query.weight', 'bert.encoder.layer.11.crossattention.self.query.bias', 'bert.encoder.layer.11.crossattention.self.key.weight', 'bert.encoder.layer.11.crossattention.self.key.bias', 'bert.encoder.layer.11.crossattention.self.value.weight', 'bert.encoder.layer.11.crossattention.self.value.bias', 'bert.encoder.layer.11.crossattention.output.dense.weight', 'bert.encoder.layer.11.crossattention.output.dense.bias', 'bert.encoder.layer.11.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.11.crossattention.output.LayerNorm.bias', 'cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# set EncoderDecoderModel\n",
    "model = EncoderDecoderModel.from_encoder_decoder_pretrained(\"bert-base-uncased\", \"bert-base-uncased\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train token classification for whether it need to be changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForTokenClassification\n",
    "import torch\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForTokenClassification.from_pretrained('bert-base-uncased', return_dict=True)\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "labels = torch.tensor([1] * inputs[\"input_ids\"].size(1)).unsqueeze(0)  # Batch size 1\n",
    "outputs = model(**inputs, labels=labels)\n",
    "# loss = outputs.loss\n",
    "# logits = outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TokenClassifierOutput(loss=tensor(0.8729, grad_fn=<NllLossBackward>), logits=tensor([[[ 0.0837, -0.0871],\n",
       "         [ 0.0103,  0.0035],\n",
       "         [ 0.3324, -0.1649],\n",
       "         [ 0.1886,  0.1879],\n",
       "         [ 0.5739, -0.0223],\n",
       "         [ 0.2863, -0.1605],\n",
       "         [ 0.5136, -0.2981],\n",
       "         [-0.2135, -0.1934]]], grad_fn=<AddBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils_ner'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-dcfd04fed309>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mset_seed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m )\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils_ner\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSplit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTokenClassificationDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTokenClassificationTask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils_ner'"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from dataclasses import dataclass, field\n",
    "from importlib import import_module\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "from seqeval.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from torch import nn\n",
    "\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoTokenizer,\n",
    "    EvalPrediction,\n",
    "    HfArgumentParser,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "from utils_ner import Split, TokenClassificationDataset, TokenClassificationTask\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n",
    "    \"\"\"\n",
    "\n",
    "    model_name_or_path: str = field(\n",
    "        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n",
    "    )\n",
    "    config_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n",
    "    )\n",
    "    task_type: Optional[str] = field(\n",
    "        default=\"NER\", metadata={\"help\": \"Task type to fine tune in training (e.g. NER, POS, etc)\"}\n",
    "    )\n",
    "    tokenizer_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n",
    "    )\n",
    "    use_fast: bool = field(default=False, metadata={\"help\": \"Set this flag to use fast tokenization.\"})\n",
    "    # If you want to tweak more attributes on your tokenizer, you should do it in a distinct script,\n",
    "    # or just modify its tokenizer_config.json.\n",
    "    cache_dir: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Where do you want to store the pretrained models downloaded from s3\"}\n",
    "    )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataTrainingArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
    "    \"\"\"\n",
    "\n",
    "    data_dir: str = field(\n",
    "        metadata={\"help\": \"The input data dir. Should contain the .txt files for a CoNLL-2003-formatted task.\"}\n",
    "    )\n",
    "    labels: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"Path to a file containing all labels. If not specified, CoNLL-2003 labels are used.\"},\n",
    "    )\n",
    "    max_seq_length: int = field(\n",
    "        default=128,\n",
    "        metadata={\n",
    "            \"help\": \"The maximum total input sequence length after tokenization. Sequences longer \"\n",
    "            \"than this will be truncated, sequences shorter will be padded.\"\n",
    "        },\n",
    "    )\n",
    "    overwrite_cache: bool = field(\n",
    "        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n",
    "    )\n",
    "\n",
    "\n",
    "def main():\n",
    "    # See all possible arguments in src/transformers/training_args.py\n",
    "    # or by passing the --help flag to this script.\n",
    "    # We now keep distinct sets of args, for a cleaner separation of concerns.\n",
    "\n",
    "    model_args = ModelArguments(model_name_or_path='bert-base-multilingual-cased', \n",
    "                                config_name=None,\n",
    "                                task_type='NER', \n",
    "                                tokenizer_name=None,\n",
    "                                use_fast=False,\n",
    "                                cache_dir=None)\n",
    "    data_args = DataTrainingArguments(data_dir='.', \n",
    "                                      labels='./labels.txt', \n",
    "                                      max_seq_length=128,\n",
    "                                      overwrite_cache=False)\n",
    "    training_args = TrainingArguments(output_dir='germeval-model',\n",
    "                                      overwrite_output_dir=False, \n",
    "                                      do_train=True,\n",
    "                                      do_eval=True, \n",
    "                                      do_predict=True,\n",
    "                                      evaluate_during_training=False, \n",
    "                                      prediction_loss_only=False,\n",
    "                                      per_device_train_batch_size=8, \n",
    "                                      per_device_eval_batch_size=8, \n",
    "                                      per_gpu_train_batch_size=32, \n",
    "                                      per_gpu_eval_batch_size=None, \n",
    "                                      gradient_accumulation_steps=1, \n",
    "                                      predict_from_generate=False, \n",
    "                                      learning_rate=5e-05, \n",
    "                                      weight_decay=0.0,\n",
    "                                      adam_beta1=0.9,\n",
    "                                      adam_beta2=0.999,\n",
    "                                      adam_epsilon=1e-08,\n",
    "                                      max_grad_norm=1.0,\n",
    "                                      num_train_epochs=3.0, \n",
    "                                      max_steps=-1,\n",
    "                                      warmup_steps=0,\n",
    "                                      logging_dir='runs/Oct07_14-28-16_node002',\n",
    "                                      logging_first_step=False, \n",
    "                                      logging_steps=500,\n",
    "                                      save_steps=750, \n",
    "                                      save_total_limit=None,\n",
    "                                      no_cuda=False,\n",
    "                                      seed=1, \n",
    "                                      fp16=False, \n",
    "                                      fp16_opt_level='O1', \n",
    "                                      local_rank=-1, \n",
    "                                      tpu_num_cores=None,\n",
    "                                      tpu_metrics_debug=False,\n",
    "                                      debug=False,\n",
    "                                      dataloader_drop_last=False, \n",
    "                                      eval_steps=1000,\n",
    "                                      past_index=-1,\n",
    "                                      run_name=None, \n",
    "                                      disable_tqdm=False,\n",
    "                                      remove_unused_columns=True,\n",
    "                                      label_names=None)\n",
    "\n",
    "\n",
    "    if (\n",
    "        os.path.exists(training_args.output_dir)\n",
    "        and os.listdir(training_args.output_dir)\n",
    "        and training_args.do_train\n",
    "        and not training_args.overwrite_output_dir\n",
    "    ):\n",
    "        raise ValueError(\n",
    "            f\"Output directory ({training_args.output_dir}) already exists and is not empty. Use --overwrite_output_dir to overcome.\"\n",
    "        )\n",
    "\n",
    "    module = import_module(\"tasks\")\n",
    "    try:\n",
    "        token_classification_task_clazz = getattr(module, model_args.task_type)\n",
    "        token_classification_task: TokenClassificationTask = token_classification_task_clazz()\n",
    "    except AttributeError:\n",
    "        raise ValueError(\n",
    "            f\"Task {model_args.task_type} needs to be defined as a TokenClassificationTask subclass in {module}. \"\n",
    "            f\"Available tasks classes are: {TokenClassificationTask.__subclasses__()}\"\n",
    "        )\n",
    "\n",
    "    # Setup logging\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO if training_args.local_rank in [-1, 0] else logging.WARN,\n",
    "    )\n",
    "    logger.warning(\n",
    "        \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n",
    "        training_args.local_rank,\n",
    "        training_args.device,\n",
    "        training_args.n_gpu,\n",
    "        bool(training_args.local_rank != -1),\n",
    "        training_args.fp16,\n",
    "    )\n",
    "    logger.info(\"Training/evaluation parameters %s\", training_args)\n",
    "\n",
    "    # Set seed\n",
    "    set_seed(training_args.seed)\n",
    "\n",
    "    # Prepare CONLL-2003 task\n",
    "    labels = token_classification_task.get_labels(data_args.labels)\n",
    "    label_map: Dict[int, str] = {i: label for i, label in enumerate(labels)}\n",
    "    num_labels = len(labels)\n",
    "\n",
    "    # Load pretrained model and tokenizer\n",
    "    #\n",
    "    # Distributed training:\n",
    "    # The .from_pretrained methods guarantee that only one local process can concurrently\n",
    "    # download model & vocab.\n",
    "\n",
    "    config = AutoConfig.from_pretrained(\n",
    "        model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n",
    "        num_labels=num_labels,\n",
    "        id2label=label_map,\n",
    "        label2id={label: i for i, label in enumerate(labels)},\n",
    "        cache_dir=model_args.cache_dir,\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,\n",
    "        cache_dir=model_args.cache_dir,\n",
    "        use_fast=model_args.use_fast,\n",
    "    )\n",
    "    model = AutoModelForTokenClassification.from_pretrained(\n",
    "        model_args.model_name_or_path,\n",
    "        from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n",
    "        config=config,\n",
    "        cache_dir=model_args.cache_dir,\n",
    "    )\n",
    "\n",
    "    # Get datasets\n",
    "    train_dataset = (\n",
    "        TokenClassificationDataset(\n",
    "            token_classification_task=token_classification_task,\n",
    "            data_dir=data_args.data_dir,\n",
    "            tokenizer=tokenizer,\n",
    "            labels=labels,\n",
    "            model_type=config.model_type,\n",
    "            max_seq_length=data_args.max_seq_length,\n",
    "            overwrite_cache=data_args.overwrite_cache,\n",
    "            mode=Split.train,\n",
    "        )\n",
    "        if training_args.do_train\n",
    "        else None\n",
    "    )\n",
    "    eval_dataset = (\n",
    "        TokenClassificationDataset(\n",
    "            token_classification_task=token_classification_task,\n",
    "            data_dir=data_args.data_dir,\n",
    "            tokenizer=tokenizer,\n",
    "            labels=labels,\n",
    "            model_type=config.model_type,\n",
    "            max_seq_length=data_args.max_seq_length,\n",
    "            overwrite_cache=data_args.overwrite_cache,\n",
    "            mode=Split.dev,\n",
    "        )\n",
    "        if training_args.do_eval\n",
    "        else None\n",
    "    )\n",
    "\n",
    "    def align_predictions(predictions: np.ndarray, label_ids: np.ndarray) -> Tuple[List[int], List[int]]:\n",
    "        preds = np.argmax(predictions, axis=2)\n",
    "\n",
    "        batch_size, seq_len = preds.shape\n",
    "\n",
    "        out_label_list = [[] for _ in range(batch_size)]\n",
    "        preds_list = [[] for _ in range(batch_size)]\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            for j in range(seq_len):\n",
    "                if label_ids[i, j] != nn.CrossEntropyLoss().ignore_index:\n",
    "                    out_label_list[i].append(label_map[label_ids[i][j]])\n",
    "                    preds_list[i].append(label_map[preds[i][j]])\n",
    "\n",
    "        return preds_list, out_label_list\n",
    "\n",
    "    def compute_metrics(p: EvalPrediction) -> Dict:\n",
    "        preds_list, out_label_list = align_predictions(p.predictions, p.label_ids)\n",
    "        return {\n",
    "            \"accuracy_score\": accuracy_score(out_label_list, preds_list),\n",
    "            \"precision\": precision_score(out_label_list, preds_list),\n",
    "            \"recall\": recall_score(out_label_list, preds_list),\n",
    "            \"f1\": f1_score(out_label_list, preds_list),\n",
    "        }\n",
    "\n",
    "    # Initialize our Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    # Training\n",
    "    if training_args.do_train:\n",
    "        trainer.train(\n",
    "            model_path=model_args.model_name_or_path if os.path.isdir(model_args.model_name_or_path) else None\n",
    "        )\n",
    "        trainer.save_model()\n",
    "        # For convenience, we also re-save the tokenizer to the same directory,\n",
    "        # so that you can share your model easily on huggingface.co/models =)\n",
    "        if trainer.is_world_master():\n",
    "            tokenizer.save_pretrained(training_args.output_dir)\n",
    "\n",
    "    # Evaluation\n",
    "    results = {}\n",
    "    if training_args.do_eval:\n",
    "        logger.info(\"*** Evaluate ***\")\n",
    "\n",
    "        result = trainer.evaluate()\n",
    "\n",
    "        output_eval_file = os.path.join(training_args.output_dir, \"eval_results.txt\")\n",
    "        if trainer.is_world_master():\n",
    "            with open(output_eval_file, \"w\") as writer:\n",
    "                logger.info(\"***** Eval results *****\")\n",
    "                for key, value in result.items():\n",
    "                    logger.info(\"  %s = %s\", key, value)\n",
    "                    writer.write(\"%s = %s\\n\" % (key, value))\n",
    "\n",
    "            results.update(result)\n",
    "\n",
    "    # Predict\n",
    "    if training_args.do_predict:\n",
    "        test_dataset = TokenClassificationDataset(\n",
    "            token_classification_task=token_classification_task,\n",
    "            data_dir=data_args.data_dir,\n",
    "            tokenizer=tokenizer,\n",
    "            labels=labels,\n",
    "            model_type=config.model_type,\n",
    "            max_seq_length=data_args.max_seq_length,\n",
    "            overwrite_cache=data_args.overwrite_cache,\n",
    "            mode=Split.test,\n",
    "        )\n",
    "\n",
    "        predictions, label_ids, metrics = trainer.predict(test_dataset)\n",
    "        preds_list, _ = align_predictions(predictions, label_ids)\n",
    "\n",
    "        output_test_results_file = os.path.join(training_args.output_dir, \"test_results.txt\")\n",
    "        if trainer.is_world_master():\n",
    "            with open(output_test_results_file, \"w\") as writer:\n",
    "                for key, value in metrics.items():\n",
    "                    logger.info(\"  %s = %s\", key, value)\n",
    "                    writer.write(\"%s = %s\\n\" % (key, value))\n",
    "\n",
    "        # Save predictions\n",
    "        output_test_predictions_file = os.path.join(training_args.output_dir, \"test_predictions.txt\")\n",
    "        if trainer.is_world_master():\n",
    "            with open(output_test_predictions_file, \"w\") as writer:\n",
    "                with open(os.path.join(data_args.data_dir, \"test.txt\"), \"r\") as f:\n",
    "                    token_classification_task.write_predictions_to_file(writer, f, preds_list)\n",
    "\n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# char-based model --> too slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EncoderDecoderModel, Trainer, TrainingArguments, AutoTokenizer, set_seed\n",
    "import torch\n",
    "\n",
    "# Encoding\n",
    "def encode(list_of_strings, pad_token_id=0):\n",
    "    max_length = max([len(string) for string in list_of_strings])\n",
    "\n",
    "    # create emtpy tensors\n",
    "    attention_masks = torch.zeros((len(list_of_strings), max_length), dtype=torch.long)\n",
    "    input_ids = torch.full((len(list_of_strings), max_length), pad_token_id, dtype=torch.long)\n",
    "\n",
    "    for idx, string in enumerate(list_of_strings):\n",
    "        # make sure string is in byte format\n",
    "        if not isinstance(string, bytes):\n",
    "            string = str.encode(string)\n",
    "\n",
    "        input_ids[idx, :len(string)] = torch.tensor([x + 2 for x in string])\n",
    "        attention_masks[idx, :len(string)] = 1\n",
    "\n",
    "    return input_ids, attention_masks\n",
    "    \n",
    "# Decoding\n",
    "def decode(outputs_ids):\n",
    "    decoded_outputs = []\n",
    "    for output_ids in outputs_ids.tolist():\n",
    "        # transform id back to char IDs < 2 are simply transformed to \"\"\n",
    "        decoded_outputs.append(\"\".join([chr(x - 2) if x > 1 else \"\" for x in output_ids]))\n",
    "    return decoded_outputs\n",
    "\n",
    "from transformers import ReformerModelWithLMHead\n",
    "\n",
    "model = ReformerModelWithLMHead.from_pretrained(\"google/reformer-enwik8\")\n",
    "encoded, attention_masks = encode([\"In 1965, Brooks left IBM to found the Department of\"])\n",
    "decode(model.generate(encoded, do_sample=True, max_length=150))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 查看tweet-copy任务在各个长度上能达到的最高点，同样查看tweet-norm任务的结果\n",
    "2. 查看tweet-copy作为Intermediate fine-tuning任务是否对tweet-norm有帮助\n",
    "3. 查看从小到大增加tweet-copy的长度能否帮助提升大长度上的结果\n",
    "4. 查看beam-num对copy和norm任务的影响\n",
    "5. 看微信推送增加规则，比如增加一个sent2，从之前的error找到最典型的无法处理的情况，比如looooooooove，合成sent2送给model\n",
    "6. 每次eval输出20个例子看看结果\n",
    "----------------------------------------------\n",
    "1. 训练token cls任务.假设3能全对训练增加align-embed过后的model.如果3和4结果都合理，考虑3、4结合到一起的流程.或者改造成类似bert预训练一样的两个loss结合的任务\n",
    "2. figure out为什么bert2bert>>others\n",
    "3. 在翻译后的结果和翻译之前跑某个sentiment analysis任务\n",
    "4. 关于新metrics\n",
    "5. 预约周五讨论和下周二讨论\n",
    "---------------------------------------------\n",
    "multi-task and Intermediate fine-tuning with token cls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:filelock:Lock 46912704707920 acquired on /home/zonghaiyao/.cache/huggingface/datasets/dcd4134ec0ad23f318793c6f8b77745d97efebf4b194bcb3c1ce90f867bec0cc.a89f1fa0750909f2d149b1ecabc808fb66cb865c94bb8bbb135c55deb50da2d7.py.lock\n",
      "INFO:filelock:Lock 46912704707920 released on /home/zonghaiyao/.cache/huggingface/datasets/dcd4134ec0ad23f318793c6f8b77745d97efebf4b194bcb3c1ce90f867bec0cc.a89f1fa0750909f2d149b1ecabc808fb66cb865c94bb8bbb135c55deb50da2d7.py.lock\n",
      "INFO:filelock:Lock 46917133956304 acquired on /home/zonghaiyao/.cache/huggingface/datasets/c7db30bf448719bd2c2ee7c233832963ab2e0b85e984dda4f577016390fa0e85.7927df63b30f94ac549ad2d2e3c61c5089402aacb0ab0478007e0abfe3431378.py.lock\n",
      "INFO:filelock:Lock 46917133956304 released on /home/zonghaiyao/.cache/huggingface/datasets/c7db30bf448719bd2c2ee7c233832963ab2e0b85e984dda4f577016390fa0e85.7927df63b30f94ac549ad2d2e3c61c5089402aacb0ab0478007e0abfe3431378.py.lock\n",
      "Some weights of BertModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.embeddings.token_align_embeddings.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertLMHeadModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertLMHeadModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.embeddings.token_align_embeddings.weight', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.self.query.bias', 'bert.encoder.layer.1.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.self.value.bias', 'bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.3.crossattention.self.query.weight', 'bert.encoder.layer.3.crossattention.self.query.bias', 'bert.encoder.layer.3.crossattention.self.key.weight', 'bert.encoder.layer.3.crossattention.self.key.bias', 'bert.encoder.layer.3.crossattention.self.value.weight', 'bert.encoder.layer.3.crossattention.self.value.bias', 'bert.encoder.layer.3.crossattention.output.dense.weight', 'bert.encoder.layer.3.crossattention.output.dense.bias', 'bert.encoder.layer.3.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.3.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.5.crossattention.self.query.weight', 'bert.encoder.layer.5.crossattention.self.query.bias', 'bert.encoder.layer.5.crossattention.self.key.weight', 'bert.encoder.layer.5.crossattention.self.key.bias', 'bert.encoder.layer.5.crossattention.self.value.weight', 'bert.encoder.layer.5.crossattention.self.value.bias', 'bert.encoder.layer.5.crossattention.output.dense.weight', 'bert.encoder.layer.5.crossattention.output.dense.bias', 'bert.encoder.layer.5.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.5.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.7.crossattention.self.query.weight', 'bert.encoder.layer.7.crossattention.self.query.bias', 'bert.encoder.layer.7.crossattention.self.key.weight', 'bert.encoder.layer.7.crossattention.self.key.bias', 'bert.encoder.layer.7.crossattention.self.value.weight', 'bert.encoder.layer.7.crossattention.self.value.bias', 'bert.encoder.layer.7.crossattention.output.dense.weight', 'bert.encoder.layer.7.crossattention.output.dense.bias', 'bert.encoder.layer.7.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.7.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.9.crossattention.self.query.weight', 'bert.encoder.layer.9.crossattention.self.query.bias', 'bert.encoder.layer.9.crossattention.self.key.weight', 'bert.encoder.layer.9.crossattention.self.key.bias', 'bert.encoder.layer.9.crossattention.self.value.weight', 'bert.encoder.layer.9.crossattention.self.value.bias', 'bert.encoder.layer.9.crossattention.output.dense.weight', 'bert.encoder.layer.9.crossattention.output.dense.bias', 'bert.encoder.layer.9.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.9.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.11.crossattention.self.query.weight', 'bert.encoder.layer.11.crossattention.self.query.bias', 'bert.encoder.layer.11.crossattention.self.key.weight', 'bert.encoder.layer.11.crossattention.self.key.bias', 'bert.encoder.layer.11.crossattention.self.value.weight', 'bert.encoder.layer.11.crossattention.self.value.bias', 'bert.encoder.layer.11.crossattention.output.dense.weight', 'bert.encoder.layer.11.crossattention.output.dense.bias', 'bert.encoder.layer.11.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.11.crossattention.output.LayerNorm.bias', 'cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import nlp\n",
    "import logging\n",
    "from datasets import load_metric\n",
    "from transformers import EncoderDecoderModel, Trainer, TrainingArguments, AutoTokenizer, EncoderDecoderConfig\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "from tweetNormalizer import normalizeTweet\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from emoji import demojize\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "nltk_tokenizer = TweetTokenizer()\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "SAME_INOUTPUT_RATE = 0\n",
    "\n",
    "TWEET_COPY_TASK = 1\n",
    "COPY_TASK_MAX_LEN = 128\n",
    "\n",
    "NORM_TASK_MAX_LEN = 128 #128 means all here, since max token num is 128\n",
    "\n",
    "FINE_TUNING_ON_TWEET_COPY = 0\n",
    "TWEET_COPY_MODEL_PATH = './models/bert2bert_share/tweetcopy_lenall/checkpoint-18000'\n",
    "\n",
    "#change 6 places\n",
    "\n",
    "#bert-base-uncased, gpt2, roberta-base, vinai/bertweet-base, google/electra-base-discriminator(only encoder)\n",
    "RUN_NAME=\"bert2bert_notebook\"\n",
    "# if FINE_TUNING_ON_TWEET_COPY, ENCODER and DECODER should be the same with copy task\n",
    "ENCODER = \"bert-base-uncased\"\n",
    "DECODER = \"bert-base-uncased\"\n",
    "tie_ENCODER_DECODER=True\n",
    "OUTPUT_DIR=\"./models/\"+RUN_NAME+\"/2/\"\n",
    "\n",
    "RUN_NAME=\"bert2bert_notebook\"\n",
    "RUN_NAME = \"zonghaiyao tweetcopy \" + RUN_NAME\n",
    "\n",
    "batch_size = 16   # set batch size here\n",
    "encoder_length = 128\n",
    "decoder_length = 128\n",
    "\n",
    "PATH_TO_TRAIN_DATA = \"WNUT2015_dataset/train_data.json\"\n",
    "PATH_TO_VAL_DATA = \"WNUT2015_dataset/test_truth.json\"\n",
    "is_alignEmbed =False\n",
    "\n",
    "rouge = load_metric('rouge', experiment_id=9)\n",
    "bleu = load_metric('bleu', experiment_id=9)\n",
    "\n",
    "#---------------------------------------------------------------------------------------------\n",
    "# encoder tokenizer\n",
    "encoder_tokenizer = AutoTokenizer.from_pretrained(ENCODER)\n",
    "\n",
    "if ENCODER==\"bert-base-uncased\" or \"google/electra-base-discriminator\":\n",
    "    # CLS token will work as BOS token, SEP token will work as EOS token\n",
    "    encoder_tokenizer.bos_token = encoder_tokenizer.cls_token\n",
    "    encoder_tokenizer.eos_token = encoder_tokenizer.sep_token\n",
    "    \n",
    "# decoder tokenizer\n",
    "if DECODER==\"gpt2\":\n",
    "    # make sure GPT2 appends EOS in begin and end\n",
    "    def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n",
    "        outputs = [self.bos_token_id] + token_ids_0 + [self.eos_token_id]\n",
    "        return outputs\n",
    "    \n",
    "    AutoTokenizer.build_inputs_with_special_tokens = build_inputs_with_special_tokens\n",
    "    decoder_tokenizer = AutoTokenizer.from_pretrained(DECODER)\n",
    "    # set pad_token_id to unk_token_id -> be careful here as unk_token_id == eos_token_id == bos_token_id\n",
    "    decoder_tokenizer.pad_token = decoder_tokenizer.unk_token\n",
    "else:   \n",
    "    decoder_tokenizer = AutoTokenizer.from_pretrained(DECODER)\n",
    "\n",
    "if DECODER==\"bert-base-uncased\":\n",
    "    # CLS token will work as BOS token, SEP token will work as EOS token\n",
    "    decoder_tokenizer.bos_token = decoder_tokenizer.cls_token\n",
    "    decoder_tokenizer.eos_token = decoder_tokenizer.sep_token\n",
    "    \n",
    "# set EncoderDecoderModel\n",
    "if FINE_TUNING_ON_TWEET_COPY == 1:\n",
    "    encoder_decoder_config = EncoderDecoderConfig.from_pretrained(TWEET_COPY_MODEL_PATH)\n",
    "    model = EncoderDecoderModel.from_pretrained(TWEET_COPY_MODEL_PATH, config=encoder_decoder_config)\n",
    "else:\n",
    "    model = EncoderDecoderModel.from_encoder_decoder_pretrained(ENCODER, DECODER, tie_encoder_decoder = tie_ENCODER_DECODER)\n",
    "\n",
    "# set decoding params\n",
    "model.config.decoder_start_token_id = decoder_tokenizer.bos_token_id\n",
    "model.config.eos_token_id = decoder_tokenizer.eos_token_id\n",
    "model.config.max_length = decoder_length\n",
    "model.config.min_length = 0\n",
    "model.early_stopping = True\n",
    "\n",
    "# will num_beams==1 good for these two task?\n",
    "model.num_beams = 4\n",
    "# sometime it will repeat something multi times in tweet, so should we change this ?\n",
    "model.config.no_repeat_ngram_size = 3\n",
    "# this may be another thing can be check\n",
    "model.length_penalty = 2.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------------------------\n",
    "# load train and validation data\n",
    "train_dataset = pd.read_json(PATH_TO_TRAIN_DATA, orient=\"records\")\n",
    "val_dataset = pd.read_json(PATH_TO_VAL_DATA, orient=\"records\")\n",
    "\n",
    "train_dataset = train_dataset.iloc[:100]\n",
    "val_dataset = val_dataset.iloc[:100]\n",
    "\n",
    "#---------------------------------\n",
    "#do some normalization ourselves\n",
    "def norm(token):\n",
    "    if token.lower().startswith(\"@\"):\n",
    "        return \"username\"\n",
    "    elif token.lower().startswith('#'):\n",
    "        return \"hashtag\"\n",
    "    elif token.lower().startswith(\"http\") or token.lower().startswith(\"www\"):\n",
    "        return \"httpurl\"\n",
    "    else:\n",
    "        return token.replace(\"’\", \"'\").replace(\"…\", \"...\")\n",
    "\n",
    "def pre_pocessing_input(x):\n",
    "    result = []\n",
    "    token_count = 0\n",
    "    for item in x:\n",
    "        item = norm(item)\n",
    "        #the reason for encode+decode is \"?!!\" need to be \"? ! !\"\n",
    "        input_ids = encoder_tokenizer(item).input_ids\n",
    "        item = encoder_tokenizer.decode(input_ids, skip_special_tokens=True)\n",
    "        result.append(item)\n",
    "        token_count += 1\n",
    "        if TWEET_COPY_TASK == 1 and token_count == COPY_TASK_MAX_LEN:\n",
    "            break\n",
    "        if TWEET_COPY_TASK == 0 and token_count == NORM_TASK_MAX_LEN:\n",
    "            break\n",
    "    return result\n",
    "\n",
    "def pre_pocessing_output(x):\n",
    "    result = []\n",
    "    token_count = 0\n",
    "    for item in x:\n",
    "        item = norm(item)\n",
    "        #the reason for encode+decode is \"?!!\" need to be \"? ! !\"\n",
    "        input_ids = decoder_tokenizer(item).input_ids\n",
    "        item = decoder_tokenizer.decode(input_ids, skip_special_tokens=True)\n",
    "        result.append(item)\n",
    "        token_count += 1\n",
    "        if TWEET_COPY_TASK == 1 and token_count == COPY_TASK_MAX_LEN:\n",
    "            break\n",
    "        if TWEET_COPY_TASK == 0 and token_count == NORM_TASK_MAX_LEN:\n",
    "            break\n",
    "    return result\n",
    "\n",
    "train_dataset['input'] = train_dataset['input'].apply(pre_pocessing_input)\n",
    "train_dataset['output'] = train_dataset['output'].apply(pre_pocessing_output)\n",
    "val_dataset['input'] = val_dataset['input'].apply(pre_pocessing_input)\n",
    "val_dataset['output'] = val_dataset['output'].apply(pre_pocessing_output)\n",
    "#-----------------------------------------\n",
    "#check if it is tweet-copy task\n",
    "if TWEET_COPY_TASK == 1:\n",
    "    train_dataset_copy = train_dataset.copy()\n",
    "    \n",
    "    make_input_output_same = lambda x: x['output'].copy()\n",
    "    train_dataset['input'] = train_dataset.apply(make_input_output_same, axis=1)\n",
    "    val_dataset['input'] = val_dataset.apply(make_input_output_same, axis=1)\n",
    "    \n",
    "    make_input_output_same = lambda x: x['input'].copy()\n",
    "    train_dataset_copy['output'] = train_dataset_copy.apply(make_input_output_same, axis=1)\n",
    "    \n",
    "    train_dataset = train_dataset.append(train_dataset_copy, ignore_index=True)\n",
    "    train_dataset = train_dataset.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "#do some data augumentation\n",
    "elif SAME_INOUTPUT_RATE > 0: \n",
    "    train_dataset_no_chage_data = train_dataset.copy()\n",
    "    train_dataset_no_chage_data = train_dataset_no_chage_data.sample(frac=1, random_state=SEED)\n",
    "    origin_input_size = train_dataset_no_chage_data.shape[0]\n",
    "    train_dataset_no_chage_data = train_dataset_no_chage_data.iloc[:int(SAME_INOUTPUT_RATE * origin_input_size)]\n",
    "    make_input_output_same = lambda x: x['output'].copy()\n",
    "    train_dataset_no_chage_data['input'] = train_dataset_no_chage_data.apply(make_input_output_same, axis=1)\n",
    "    train_dataset = train_dataset.append(train_dataset_no_chage_data, ignore_index=True)\n",
    "    train_dataset = train_dataset.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "#-------------------------------------------\n",
    "#make sentence for token list\n",
    "make_sentence = lambda x : \" \".join(x).lower()\n",
    "\n",
    "train_dataset['input_sentence'] = train_dataset['input'].apply(make_sentence)\n",
    "train_dataset['output_sentence'] = train_dataset['output'].apply(make_sentence)\n",
    "val_dataset['input_sentence'] = val_dataset['input'].apply(make_sentence)\n",
    "val_dataset['output_sentence'] = val_dataset['output'].apply(make_sentence)\n",
    "\n",
    "#------------------------------------------\n",
    "#make is_labels_need_change for previous metrics\n",
    "#also calculate token_align_ids here, only for input, so use encode_tokenizer\n",
    "labels_word2id = {}\n",
    "def make_special_input_output(x):\n",
    "    assert len(x['input']) == len(x['output'])\n",
    "    \n",
    "    input_token = []\n",
    "    output_token = []\n",
    "    \n",
    "    for i in range(len(x['input'])):\n",
    "        input_token.append(x['input'][i].split())\n",
    "        output_token.append(x['output'][i].split())\n",
    "\n",
    "    #then we make is_labels_need_change\n",
    "    result = {}\n",
    "    is_labels_need_change = []\n",
    "    #align_ids for bos token is 0\n",
    "    align_ids = [0]\n",
    "    for i in range(len(input_token)):\n",
    "        if len(input_token[i]) == len(output_token[i]):\n",
    "            for j in range(len(output_token[i])):\n",
    "                #add token into labels_word2id dict\n",
    "                if output_token[i][j] not in labels_word2id.keys():\n",
    "                    labels_word2id[output_token[i][j]] = len(labels_word2id)\n",
    "                #they are the same, no need change, and align_ids + 0(keep) * ids_num\n",
    "                if output_token[i][j].lower() == input_token[i][j].lower():\n",
    "                    is_labels_need_change.append([labels_word2id[output_token[i][j]], 0])\n",
    "                    length = len(encoder_tokenizer(input_token[i][j].lower(), add_special_tokens=False).input_ids)\n",
    "                    align_ids.extend(list(np.zeros(length, dtype = np.int8)))\n",
    "                #they are diff, need change, and align_ids + 1(norm) * ids_num\n",
    "                else:\n",
    "                    is_labels_need_change.append([labels_word2id[output_token[i][j]], 1])\n",
    "                    length = len(encoder_tokenizer(input_token[i][j].lower(), add_special_tokens=False).input_ids)\n",
    "                    align_ids.extend(list(np.ones(length, dtype = np.int8)))\n",
    "        else:\n",
    "            for j in range(len(output_token[i])):\n",
    "                #add token into labels_word2id dict\n",
    "                if output_token[i][j] not in labels_word2id.keys():\n",
    "                    labels_word2id[output_token[i][j]] = len(labels_word2id)\n",
    "                #they are diff, need change\n",
    "                is_labels_need_change.append([labels_word2id[output_token[i][j]], 1])\n",
    "            for j in range(len(input_token[i])):\n",
    "                #they are diff, align_ids + 1(norm) * ids_num\n",
    "                length = len(encoder_tokenizer(input_token[i][j].lower(), add_special_tokens=False).input_ids)\n",
    "                align_ids.extend(list(np.ones(length, dtype = np.int8)))\n",
    "    \n",
    "    #align_ids for eos token is 0\n",
    "    align_ids.append(0)\n",
    "    #pad 0 to max_encoder_length\n",
    "    while len(align_ids) < encoder_length : align_ids.append(0)\n",
    "    \n",
    "    return is_labels_need_change, align_ids\n",
    "\n",
    "train_dataset[['is_labels_need_change', 'align_ids']] = train_dataset.apply(make_special_input_output, axis=1, result_type=\"expand\")\n",
    "val_dataset[['is_labels_need_change', 'align_ids']] = val_dataset.apply(make_special_input_output, axis=1, result_type=\"expand\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'miss', 'you', 'my', 'bie', '!', 'where', 'you', 'wanna', 'out', 'with', 'me', '?', 'have', 'a', 'wonderful', 'day', 'like', 'a', 'swan', 'day', '. .', 'haha', '. .']\n",
      "['i', 'miss', 'u', 'my', 'bie', '!', 'where', 'u', 'wanna', 'out', 'wif', 'me', '?', 'have', 'a', 'wonderful', 'day', 'like', 'a', 'swan', 'day', '. .', 'haha', '. .']\n",
      "i miss u my bie ! where u wanna out wif me ? have a wonderful day like a swan day . . haha . .\n",
      "i miss you my bie ! where you wanna out with me ? have a wonderful day like a swan day . . haha . .\n",
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[21, 0], [22, 0], [11, 1], [23, 0], [24, 0], [25, 0], [26, 0], [11, 1], [27, 0], [28, 0], [29, 1], [30, 0], [31, 0], [15, 0], [16, 0], [32, 0], [33, 0], [34, 0], [16, 0], [35, 0], [33, 0], [5, 0], [5, 0], [36, 0], [5, 0], [5, 0]]\n"
     ]
    }
   ],
   "source": [
    "# for check the correctness of pre-pocessing\n",
    "i = 2\n",
    "\n",
    "print(train_dataset.iloc[i]['output'])\n",
    "print(train_dataset.iloc[i]['input'])\n",
    "print(train_dataset.iloc[i]['input_sentence'])\n",
    "print(train_dataset.iloc[i]['output_sentence'])\n",
    "print(train_dataset.iloc[i][\"align_ids\"])\n",
    "print(train_dataset.iloc[i][\"is_labels_need_change\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_pandas(train_dataset)\n",
    "val_dataset = Dataset.from_pandas(val_dataset)\n",
    "\n",
    "labels_id2word = {v: k for k, v in labels_word2id.items()}\n",
    "#-------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "global batch_copy\n",
    "batch_copy = None\n",
    "\n",
    "# map data correctly\n",
    "def map_to_encoder_decoder_inputs(batch):\n",
    "    global batch_copy\n",
    "    batch_copy = batch\n",
    "    # Tokenizer will automatically set [BOS] <text> [EOS]\n",
    "    inputs = encoder_tokenizer(batch[\"input_sentence\"], padding=\"max_length\", truncation=True, max_length=encoder_length)\n",
    "    outputs = decoder_tokenizer(batch[\"output_sentence\"], padding=\"max_length\", truncation=True, max_length=decoder_length)\n",
    "\n",
    "    batch[\"input_ids\"] = inputs.input_ids\n",
    "    if is_alignEmbed:\n",
    "        batch[\"token_align_ids\"] = batch[\"align_ids\"]\n",
    "   \n",
    "    batch[\"is_labels_need_change\"] = batch[\"is_labels_need_change\"]\n",
    "    batch[\"attention_mask\"] = inputs.attention_mask\n",
    "\n",
    "    batch[\"decoder_input_ids\"] = outputs.input_ids\n",
    "    batch[\"labels\"] = outputs.input_ids.copy()\n",
    "    batch[\"decoder_attention_mask\"] = outputs.attention_mask\n",
    "    \n",
    "    \n",
    "    if DECODER==\"gpt2\":\n",
    "        # complicated list comprehension here because pad_token_id alone is not good enough to know whether label should be excluded or not\n",
    "        batch[\"labels\"] = [\n",
    "            [-100 if mask == 0 else token for mask, token in mask_and_tokens] for mask_and_tokens in [zip(masks, labels) for masks, labels in zip(batch[\"decoder_attention_mask\"], batch[\"labels\"])]\n",
    "        ]\n",
    "    else:\n",
    "        # mask loss for padding\n",
    "        batch[\"labels\"] = [\n",
    "            [-100 if token == decoder_tokenizer.pad_token_id else token for token in labels] for labels in batch[\"labels\"]\n",
    "        ]\n",
    "    \n",
    "\n",
    "    assert all([len(x) == encoder_length for x in inputs.input_ids])\n",
    "    assert all([len(x) == decoder_length for x in outputs.input_ids])\n",
    "    \n",
    "    batch_copy = batch\n",
    "    \n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75978868759d45fea79b6e318fbc4152",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b53bab02e1f14f6797b70bb216091575",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# make train dataset ready\n",
    "train_dataset = train_dataset.map(\n",
    "    map_to_encoder_decoder_inputs, batched=True, batch_size=batch_size, remove_columns=[\"input_sentence\", \"output_sentence\"],\n",
    ")\n",
    "\n",
    "if is_alignEmbed:\n",
    "    train_dataset.set_format(\n",
    "        type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"decoder_input_ids\", \"decoder_attention_mask\", \"labels\", \"is_labels_need_change\", \"token_align_ids\"],\n",
    "    )\n",
    "else:\n",
    "    train_dataset.set_format(\n",
    "        type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"decoder_input_ids\", \"decoder_attention_mask\", \"labels\", \"is_labels_need_change\"],\n",
    "    )\n",
    "\n",
    "# same for validation dataset\n",
    "val_dataset = val_dataset.map(\n",
    "    map_to_encoder_decoder_inputs, batched=True, batch_size=batch_size, remove_columns=[\"input_sentence\", \"output_sentence\"],\n",
    ")\n",
    "\n",
    "if is_alignEmbed:\n",
    "    val_dataset.set_format(\n",
    "        type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"decoder_input_ids\", \"decoder_attention_mask\", \"labels\", \"is_labels_need_change\", \"token_align_ids\"],\n",
    "    )\n",
    "else:\n",
    "    val_dataset.set_format(\n",
    "        type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"decoder_input_ids\", \"decoder_attention_mask\", \"labels\", \"is_labels_need_change\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b11163bdad4455e87ef2ae2e9fd2053",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=30.0, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7e2d82c469f4600b2c5d39c47a895c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=7.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/nfs/work1/llcao/zonghaiyao/tweetNorm/datasets/src/datasets/arrow_dataset.py:835: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  return torch.tensor(x, **format_kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9a64de61cc54c9c946ddbe91647d1f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=7.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8613bd68e804284b97469e7d2c6841e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=7.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:wandb.jupyter:Failed to query for notebook name, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable\n",
      "wandb: Currently logged in as: iesl-boxes (use `wandb login --relogin` to force relogin)\n",
      "wandb: wandb version 0.10.7 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "wandb: Tracking run with wandb version 0.10.2\n",
      "wandb: Run data is saved locally in wandb/run-20201016_120539-2bjydk40\n",
      "wandb: Syncing run bert2bert_notebook\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/zonghaiyao/tweetcopy\" target=\"_blank\">https://wandb.ai/zonghaiyao/tweetcopy</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/zonghaiyao/tweetcopy/runs/2bjydk40\" target=\"_blank\">https://wandb.ai/zonghaiyao/tweetcopy/runs/2bjydk40</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'loss': 1.6223705291748047, 'learning_rate': 2e-05, 'epoch': 2.857142857142857, 'total_flos': 62957704863744, 'step': 20}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/nfs/work1/llcao/zonghaiyao/tweetNorm/transformers/src/transformers/trainer.py:1449: FutureWarning: The `_prediction_loop` method is deprecated and won't be called in a future version, define `prediction_loop` in your subclass.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec26ae345f1141d792ece36e5a61cc8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluation', max=7.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 102 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 102 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 102 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 102 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 102 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 102 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 102 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:filelock:Lock 46917466296080 acquired on /home/zonghaiyao/.cache/huggingface/metrics/rouge/default/9-1-0.arrow.lock\n",
      "INFO:filelock:Lock 46917466296080 released on /home/zonghaiyao/.cache/huggingface/metrics/rouge/default/9-1-0.arrow.lock\n",
      "INFO:filelock:Lock 46917466296272 acquired on /home/zonghaiyao/.cache/huggingface/metrics/rouge/default/9-1-0.arrow.lock\n",
      "INFO:/mnt/nfs/work1/llcao/zonghaiyao/tweetNorm/datasets/src/datasets/metric.py:Removing /home/zonghaiyao/.cache/huggingface/metrics/rouge/default/9-1-0.arrow\n",
      "INFO:filelock:Lock 46917466296272 released on /home/zonghaiyao/.cache/huggingface/metrics/rouge/default/9-1-0.arrow.lock\n",
      "INFO:filelock:Lock 46917465143504 acquired on /home/zonghaiyao/.cache/huggingface/metrics/bleu/default/9-1-0.arrow.lock\n",
      "INFO:filelock:Lock 46917465143504 released on /home/zonghaiyao/.cache/huggingface/metrics/bleu/default/9-1-0.arrow.lock\n",
      "INFO:filelock:Lock 46917466292944 acquired on /home/zonghaiyao/.cache/huggingface/metrics/bleu/default/9-1-0.arrow.lock\n",
      "INFO:/mnt/nfs/work1/llcao/zonghaiyao/tweetNorm/datasets/src/datasets/metric.py:Removing /home/zonghaiyao/.cache/huggingface/metrics/bleu/default/9-1-0.arrow\n",
      "INFO:filelock:Lock 46917466292944 released on /home/zonghaiyao/.cache/huggingface/metrics/bleu/default/9-1-0.arrow.lock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.2063550778797696, 'eval_rouge1_precision': 0.7768, 'eval_rouge1_recall': 0.7248, 'eval_rouge1_fmeasure': 0.7463, 'eval_rouge2_precision': 0.6327, 'eval_rouge2_recall': 0.5893, 'eval_rouge2_fmeasure': 0.6069, 'eval_rouge3_precision': 0.5036, 'eval_rouge3_recall': 0.4703, 'eval_rouge3_fmeasure': 0.4841, 'eval_rouge4_precision': 0.4035, 'eval_rouge4_recall': 0.379, 'eval_rouge4_fmeasure': 0.3888, 'eval_rougeL_precision': 0.7778, 'eval_rougeL_recall': 0.7247, 'eval_rougeL_fmeasure': 0.7465, 'eval_rougeLsum_precision': 0.7772, 'eval_rougeLsum_recall': 0.7234, 'eval_rougeLsum_fmeasure': 0.7453, 'eval_bleu': 0.6487, 'eval_norm_correct': 5.0, 'eval_norm_wrong': 220.0, 'eval_keep_correct': 965.0, 'eval_keep_wrong': 413.0, 'eval_correct_of_all_need_norm': 0.0222, 'eval_correct_of_all_need_keep': 0.7003, 'eval_correct_token': 0.6051, 'epoch': 2.857142857142857, 'total_flos': 62957704863744, 'step': 20}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f273751b2bb84434b33c41d97f772799",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=7.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f867d3a455794dfaba03029710956728",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=7.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63e614e5a14a41a9904d37c3b41e7866",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=7.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5597034454345703, 'learning_rate': 4e-05, 'epoch': 5.714285714285714, 'total_flos': 123363070341120, 'step': 40}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9edcfec1e3b04178a8839e7acec7a913",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluation', max=7.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 102 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 102 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 102 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 102 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 102 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 102 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 102 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:filelock:Lock 46917465200144 acquired on /home/zonghaiyao/.cache/huggingface/metrics/rouge/default/9-1-0.arrow.lock\n",
      "INFO:filelock:Lock 46917465200144 released on /home/zonghaiyao/.cache/huggingface/metrics/rouge/default/9-1-0.arrow.lock\n",
      "INFO:filelock:Lock 46917468067152 acquired on /home/zonghaiyao/.cache/huggingface/metrics/rouge/default/9-1-0.arrow.lock\n",
      "INFO:/mnt/nfs/work1/llcao/zonghaiyao/tweetNorm/datasets/src/datasets/metric.py:Removing /home/zonghaiyao/.cache/huggingface/metrics/rouge/default/9-1-0.arrow\n",
      "INFO:filelock:Lock 46917468067152 released on /home/zonghaiyao/.cache/huggingface/metrics/rouge/default/9-1-0.arrow.lock\n",
      "INFO:filelock:Lock 46917147797648 acquired on /home/zonghaiyao/.cache/huggingface/metrics/bleu/default/9-1-0.arrow.lock\n",
      "INFO:filelock:Lock 46917147797648 released on /home/zonghaiyao/.cache/huggingface/metrics/bleu/default/9-1-0.arrow.lock\n",
      "INFO:filelock:Lock 46917466366928 acquired on /home/zonghaiyao/.cache/huggingface/metrics/bleu/default/9-1-0.arrow.lock\n",
      "INFO:/mnt/nfs/work1/llcao/zonghaiyao/tweetNorm/datasets/src/datasets/metric.py:Removing /home/zonghaiyao/.cache/huggingface/metrics/bleu/default/9-1-0.arrow\n",
      "INFO:filelock:Lock 46917466366928 released on /home/zonghaiyao/.cache/huggingface/metrics/bleu/default/9-1-0.arrow.lock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.470862763268607, 'eval_rouge1_precision': 0.785, 'eval_rouge1_recall': 0.7449, 'eval_rouge1_fmeasure': 0.761, 'eval_rouge2_precision': 0.6411, 'eval_rouge2_recall': 0.6073, 'eval_rouge2_fmeasure': 0.6217, 'eval_rouge3_precision': 0.52, 'eval_rouge3_recall': 0.4942, 'eval_rouge3_fmeasure': 0.5042, 'eval_rouge4_precision': 0.432, 'eval_rouge4_recall': 0.4119, 'eval_rouge4_fmeasure': 0.4201, 'eval_rougeL_precision': 0.7844, 'eval_rougeL_recall': 0.7425, 'eval_rougeL_fmeasure': 0.7596, 'eval_rougeLsum_precision': 0.7833, 'eval_rougeLsum_recall': 0.7423, 'eval_rougeLsum_fmeasure': 0.7584, 'eval_bleu': 0.6523, 'eval_norm_correct': 32.0, 'eval_norm_wrong': 193.0, 'eval_keep_correct': 939.0, 'eval_keep_wrong': 439.0, 'eval_correct_of_all_need_norm': 0.1422, 'eval_correct_of_all_need_keep': 0.6814, 'eval_correct_token': 0.6057, 'epoch': 5.714285714285714, 'total_flos': 123363070341120, 'step': 40}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd96429b74d545b4bc1d02dd8f88ad6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=7.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28af706a3dc14dd59dfa8a0959ba70e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=7.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bea7aaab0924c3996d542253658087d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=7.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1285238265991211, 'learning_rate': 4.6875e-05, 'epoch': 8.571428571428571, 'total_flos': 183768435818496, 'step': 60}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "649bc295dc68428eb4da894f00d1e9ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluation', max=7.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 102 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 102 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 102 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 102 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 102 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 102 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 102 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:filelock:Lock 46917469358928 acquired on /home/zonghaiyao/.cache/huggingface/metrics/rouge/default/9-1-0.arrow.lock\n",
      "INFO:filelock:Lock 46917469358928 released on /home/zonghaiyao/.cache/huggingface/metrics/rouge/default/9-1-0.arrow.lock\n",
      "INFO:filelock:Lock 46917466295440 acquired on /home/zonghaiyao/.cache/huggingface/metrics/rouge/default/9-1-0.arrow.lock\n",
      "INFO:/mnt/nfs/work1/llcao/zonghaiyao/tweetNorm/datasets/src/datasets/metric.py:Removing /home/zonghaiyao/.cache/huggingface/metrics/rouge/default/9-1-0.arrow\n",
      "INFO:filelock:Lock 46917466295440 released on /home/zonghaiyao/.cache/huggingface/metrics/rouge/default/9-1-0.arrow.lock\n",
      "INFO:filelock:Lock 46917147797776 acquired on /home/zonghaiyao/.cache/huggingface/metrics/bleu/default/9-1-0.arrow.lock\n",
      "INFO:filelock:Lock 46917147797776 released on /home/zonghaiyao/.cache/huggingface/metrics/bleu/default/9-1-0.arrow.lock\n",
      "INFO:filelock:Lock 46917466403856 acquired on /home/zonghaiyao/.cache/huggingface/metrics/bleu/default/9-1-0.arrow.lock\n",
      "INFO:/mnt/nfs/work1/llcao/zonghaiyao/tweetNorm/datasets/src/datasets/metric.py:Removing /home/zonghaiyao/.cache/huggingface/metrics/bleu/default/9-1-0.arrow\n",
      "INFO:filelock:Lock 46917466403856 released on /home/zonghaiyao/.cache/huggingface/metrics/bleu/default/9-1-0.arrow.lock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3599485329219274, 'eval_rouge1_precision': 0.8017, 'eval_rouge1_recall': 0.7719, 'eval_rouge1_fmeasure': 0.7834, 'eval_rouge2_precision': 0.6712, 'eval_rouge2_recall': 0.6447, 'eval_rouge2_fmeasure': 0.6546, 'eval_rouge3_precision': 0.5558, 'eval_rouge3_recall': 0.5331, 'eval_rouge3_fmeasure': 0.5411, 'eval_rouge4_precision': 0.463, 'eval_rouge4_recall': 0.4452, 'eval_rouge4_fmeasure': 0.4522, 'eval_rougeL_precision': 0.8028, 'eval_rougeL_recall': 0.7701, 'eval_rougeL_fmeasure': 0.7831, 'eval_rougeLsum_precision': 0.8011, 'eval_rougeLsum_recall': 0.7706, 'eval_rougeLsum_fmeasure': 0.783, 'eval_bleu': 0.6805, 'eval_norm_correct': 53.0, 'eval_norm_wrong': 172.0, 'eval_keep_correct': 942.0, 'eval_keep_wrong': 436.0, 'eval_correct_of_all_need_norm': 0.2356, 'eval_correct_of_all_need_keep': 0.6836, 'eval_correct_token': 0.6207, 'epoch': 8.571428571428571, 'total_flos': 183768435818496, 'step': 60}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa5d38ca90c2453b8e7b485cf463af94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=7.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d56b2142633d4cd8b55c945f45e12b87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=7.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02af6f8d65b7407ca7403bf212cbcd5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=7.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.01428680419921875, 'learning_rate': 4.0625000000000005e-05, 'epoch': 11.428571428571429, 'total_flos': 244173801295872, 'step': 80}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe0a0b05700f4ff1b52059ff09312544",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluation', max=7.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 102 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 102 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 102 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 102 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 102 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 102 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 102 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:filelock:Lock 46917147680464 acquired on /home/zonghaiyao/.cache/huggingface/metrics/rouge/default/9-1-0.arrow.lock\n",
      "INFO:filelock:Lock 46917147680464 released on /home/zonghaiyao/.cache/huggingface/metrics/rouge/default/9-1-0.arrow.lock\n",
      "INFO:filelock:Lock 46917469717392 acquired on /home/zonghaiyao/.cache/huggingface/metrics/rouge/default/9-1-0.arrow.lock\n",
      "INFO:/mnt/nfs/work1/llcao/zonghaiyao/tweetNorm/datasets/src/datasets/metric.py:Removing /home/zonghaiyao/.cache/huggingface/metrics/rouge/default/9-1-0.arrow\n",
      "INFO:filelock:Lock 46917469717392 released on /home/zonghaiyao/.cache/huggingface/metrics/rouge/default/9-1-0.arrow.lock\n",
      "INFO:filelock:Lock 46917469359696 acquired on /home/zonghaiyao/.cache/huggingface/metrics/bleu/default/9-1-0.arrow.lock\n",
      "INFO:filelock:Lock 46917469359696 released on /home/zonghaiyao/.cache/huggingface/metrics/bleu/default/9-1-0.arrow.lock\n",
      "INFO:filelock:Lock 46917147680464 acquired on /home/zonghaiyao/.cache/huggingface/metrics/bleu/default/9-1-0.arrow.lock\n",
      "INFO:/mnt/nfs/work1/llcao/zonghaiyao/tweetNorm/datasets/src/datasets/metric.py:Removing /home/zonghaiyao/.cache/huggingface/metrics/bleu/default/9-1-0.arrow\n",
      "INFO:filelock:Lock 46917147680464 released on /home/zonghaiyao/.cache/huggingface/metrics/bleu/default/9-1-0.arrow.lock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.364357488495963, 'eval_rouge1_precision': 0.8007, 'eval_rouge1_recall': 0.7671, 'eval_rouge1_fmeasure': 0.78, 'eval_rouge2_precision': 0.6746, 'eval_rouge2_recall': 0.6444, 'eval_rouge2_fmeasure': 0.6555, 'eval_rouge3_precision': 0.5645, 'eval_rouge3_recall': 0.5387, 'eval_rouge3_fmeasure': 0.549, 'eval_rouge4_precision': 0.4736, 'eval_rouge4_recall': 0.4518, 'eval_rouge4_fmeasure': 0.4605, 'eval_rougeL_precision': 0.8006, 'eval_rougeL_recall': 0.7651, 'eval_rougeL_fmeasure': 0.779, 'eval_rougeLsum_precision': 0.8005, 'eval_rougeLsum_recall': 0.7657, 'eval_rougeLsum_fmeasure': 0.7796, 'eval_bleu': 0.679, 'eval_norm_correct': 52.0, 'eval_norm_wrong': 173.0, 'eval_keep_correct': 863.0, 'eval_keep_wrong': 515.0, 'eval_correct_of_all_need_norm': 0.2311, 'eval_correct_of_all_need_keep': 0.6263, 'eval_correct_token': 0.5708, 'epoch': 11.428571428571429, 'total_flos': 244173801295872, 'step': 80}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f0bf3e8fdd44c2988a8337a51ab2cd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=7.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "684dd5154a054c70a8d39a868c970aec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=7.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a016c12d46d1424da1e0977a2557c596",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=7.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0051708221435546875, 'learning_rate': 3.4375e-05, 'epoch': 14.285714285714286, 'total_flos': 304579166773248, 'step': 100}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd16d78c2d9d49f98a2e44dc750ba162",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluation', max=7.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 102 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 102 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 102 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 102 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 102 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 102 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 102 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:filelock:Lock 46917469832080 acquired on /home/zonghaiyao/.cache/huggingface/metrics/rouge/default/9-1-0.arrow.lock\n",
      "INFO:filelock:Lock 46917469832080 released on /home/zonghaiyao/.cache/huggingface/metrics/rouge/default/9-1-0.arrow.lock\n",
      "INFO:filelock:Lock 46917463876688 acquired on /home/zonghaiyao/.cache/huggingface/metrics/rouge/default/9-1-0.arrow.lock\n",
      "INFO:/mnt/nfs/work1/llcao/zonghaiyao/tweetNorm/datasets/src/datasets/metric.py:Removing /home/zonghaiyao/.cache/huggingface/metrics/rouge/default/9-1-0.arrow\n",
      "INFO:filelock:Lock 46917463876688 released on /home/zonghaiyao/.cache/huggingface/metrics/rouge/default/9-1-0.arrow.lock\n",
      "INFO:filelock:Lock 46917463875792 acquired on /home/zonghaiyao/.cache/huggingface/metrics/bleu/default/9-1-0.arrow.lock\n",
      "INFO:filelock:Lock 46917463875792 released on /home/zonghaiyao/.cache/huggingface/metrics/bleu/default/9-1-0.arrow.lock\n",
      "INFO:filelock:Lock 46917466480528 acquired on /home/zonghaiyao/.cache/huggingface/metrics/bleu/default/9-1-0.arrow.lock\n",
      "INFO:/mnt/nfs/work1/llcao/zonghaiyao/tweetNorm/datasets/src/datasets/metric.py:Removing /home/zonghaiyao/.cache/huggingface/metrics/bleu/default/9-1-0.arrow\n",
      "INFO:filelock:Lock 46917466480528 released on /home/zonghaiyao/.cache/huggingface/metrics/bleu/default/9-1-0.arrow.lock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3410739132336207, 'eval_rouge1_precision': 0.8085, 'eval_rouge1_recall': 0.7778, 'eval_rouge1_fmeasure': 0.7896, 'eval_rouge2_precision': 0.6855, 'eval_rouge2_recall': 0.6569, 'eval_rouge2_fmeasure': 0.6679, 'eval_rouge3_precision': 0.5758, 'eval_rouge3_recall': 0.5528, 'eval_rouge3_fmeasure': 0.5618, 'eval_rouge4_precision': 0.4872, 'eval_rouge4_recall': 0.4672, 'eval_rouge4_fmeasure': 0.4746, 'eval_rougeL_precision': 0.8074, 'eval_rougeL_recall': 0.7754, 'eval_rougeL_fmeasure': 0.7878, 'eval_rougeLsum_precision': 0.8074, 'eval_rougeLsum_recall': 0.776, 'eval_rougeLsum_fmeasure': 0.7879, 'eval_bleu': 0.693, 'eval_norm_correct': 58.0, 'eval_norm_wrong': 167.0, 'eval_keep_correct': 934.0, 'eval_keep_wrong': 444.0, 'eval_correct_of_all_need_norm': 0.2578, 'eval_correct_of_all_need_keep': 0.6778, 'eval_correct_token': 0.6188, 'epoch': 14.285714285714286, 'total_flos': 304579166773248, 'step': 100}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zonghaiyao/anaconda3/envs/tweetNorm/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef73b34cc5d14c81a0d63e1882c50802",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=7.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faa6046676dd49329b9639f661752eab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=7.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2cb205e583747faa462677ba652246d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=7.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.002156639099121094, 'learning_rate': 2.8125000000000003e-05, 'epoch': 17.142857142857142, 'total_flos': 364984532250624, 'step': 120}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d463e811eb814040a09e9c892a469f21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluation', max=7.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 102 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 102 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 102 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 102 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 102 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 102 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 102 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:filelock:Lock 46917469177616 acquired on /home/zonghaiyao/.cache/huggingface/metrics/rouge/default/9-1-0.arrow.lock\n",
      "INFO:filelock:Lock 46917469177616 released on /home/zonghaiyao/.cache/huggingface/metrics/rouge/default/9-1-0.arrow.lock\n",
      "INFO:filelock:Lock 46917469363792 acquired on /home/zonghaiyao/.cache/huggingface/metrics/rouge/default/9-1-0.arrow.lock\n",
      "INFO:/mnt/nfs/work1/llcao/zonghaiyao/tweetNorm/datasets/src/datasets/metric.py:Removing /home/zonghaiyao/.cache/huggingface/metrics/rouge/default/9-1-0.arrow\n",
      "INFO:filelock:Lock 46917469363792 released on /home/zonghaiyao/.cache/huggingface/metrics/rouge/default/9-1-0.arrow.lock\n",
      "INFO:filelock:Lock 46917464483856 acquired on /home/zonghaiyao/.cache/huggingface/metrics/bleu/default/9-1-0.arrow.lock\n",
      "INFO:filelock:Lock 46917464483856 released on /home/zonghaiyao/.cache/huggingface/metrics/bleu/default/9-1-0.arrow.lock\n",
      "INFO:filelock:Lock 46917133512656 acquired on /home/zonghaiyao/.cache/huggingface/metrics/bleu/default/9-1-0.arrow.lock\n",
      "INFO:/mnt/nfs/work1/llcao/zonghaiyao/tweetNorm/datasets/src/datasets/metric.py:Removing /home/zonghaiyao/.cache/huggingface/metrics/bleu/default/9-1-0.arrow\n",
      "INFO:filelock:Lock 46917133512656 released on /home/zonghaiyao/.cache/huggingface/metrics/bleu/default/9-1-0.arrow.lock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3494833707809448, 'eval_rouge1_precision': 0.8069, 'eval_rouge1_recall': 0.7724, 'eval_rouge1_fmeasure': 0.7857, 'eval_rouge2_precision': 0.6845, 'eval_rouge2_recall': 0.6513, 'eval_rouge2_fmeasure': 0.664, 'eval_rouge3_precision': 0.5714, 'eval_rouge3_recall': 0.5429, 'eval_rouge3_fmeasure': 0.5538, 'eval_rouge4_precision': 0.4778, 'eval_rouge4_recall': 0.4528, 'eval_rouge4_fmeasure': 0.4623, 'eval_rougeL_precision': 0.8059, 'eval_rougeL_recall': 0.7703, 'eval_rougeL_fmeasure': 0.7845, 'eval_rougeLsum_precision': 0.8059, 'eval_rougeLsum_recall': 0.7709, 'eval_rougeLsum_fmeasure': 0.7841, 'eval_bleu': 0.6856, 'eval_norm_correct': 53.0, 'eval_norm_wrong': 172.0, 'eval_keep_correct': 932.0, 'eval_keep_wrong': 446.0, 'eval_correct_of_all_need_norm': 0.2356, 'eval_correct_of_all_need_keep': 0.6763, 'eval_correct_token': 0.6145, 'epoch': 17.142857142857142, 'total_flos': 364984532250624, 'step': 120}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da19b182fed04fdb9212ed2e10f7253c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=7.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc480b752bcb429ea47d9743581910fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=7.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0021221160888671873, 'learning_rate': 2.1875e-05, 'epoch': 20.0, 'total_flos': 425389897728000, 'step': 140}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a601fbe5cdb404c84d150bca6b379b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluation', max=7.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 102 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 102 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 102 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 102 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 102 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 102 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 102 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:filelock:Lock 46917475404880 acquired on /home/zonghaiyao/.cache/huggingface/metrics/rouge/default/9-1-0.arrow.lock\n",
      "INFO:filelock:Lock 46917475404880 released on /home/zonghaiyao/.cache/huggingface/metrics/rouge/default/9-1-0.arrow.lock\n",
      "INFO:filelock:Lock 46917463876880 acquired on /home/zonghaiyao/.cache/huggingface/metrics/rouge/default/9-1-0.arrow.lock\n",
      "INFO:/mnt/nfs/work1/llcao/zonghaiyao/tweetNorm/datasets/src/datasets/metric.py:Removing /home/zonghaiyao/.cache/huggingface/metrics/rouge/default/9-1-0.arrow\n",
      "INFO:filelock:Lock 46917463876880 released on /home/zonghaiyao/.cache/huggingface/metrics/rouge/default/9-1-0.arrow.lock\n",
      "INFO:filelock:Lock 46917147725584 acquired on /home/zonghaiyao/.cache/huggingface/metrics/bleu/default/9-1-0.arrow.lock\n",
      "INFO:filelock:Lock 46917147725584 released on /home/zonghaiyao/.cache/huggingface/metrics/bleu/default/9-1-0.arrow.lock\n",
      "INFO:filelock:Lock 46917148026640 acquired on /home/zonghaiyao/.cache/huggingface/metrics/bleu/default/9-1-0.arrow.lock\n",
      "INFO:/mnt/nfs/work1/llcao/zonghaiyao/tweetNorm/datasets/src/datasets/metric.py:Removing /home/zonghaiyao/.cache/huggingface/metrics/bleu/default/9-1-0.arrow\n",
      "INFO:filelock:Lock 46917148026640 released on /home/zonghaiyao/.cache/huggingface/metrics/bleu/default/9-1-0.arrow.lock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3514773079327174, 'eval_rouge1_precision': 0.8089, 'eval_rouge1_recall': 0.7731, 'eval_rouge1_fmeasure': 0.7869, 'eval_rouge2_precision': 0.6856, 'eval_rouge2_recall': 0.6541, 'eval_rouge2_fmeasure': 0.6656, 'eval_rouge3_precision': 0.5723, 'eval_rouge3_recall': 0.5459, 'eval_rouge3_fmeasure': 0.5553, 'eval_rouge4_precision': 0.4782, 'eval_rouge4_recall': 0.4557, 'eval_rouge4_fmeasure': 0.463, 'eval_rougeL_precision': 0.8066, 'eval_rougeL_recall': 0.7709, 'eval_rougeL_fmeasure': 0.7854, 'eval_rougeLsum_precision': 0.807, 'eval_rougeLsum_recall': 0.7708, 'eval_rougeLsum_fmeasure': 0.7851, 'eval_bleu': 0.6908, 'eval_norm_correct': 55.0, 'eval_norm_wrong': 170.0, 'eval_keep_correct': 923.0, 'eval_keep_wrong': 455.0, 'eval_correct_of_all_need_norm': 0.2444, 'eval_correct_of_all_need_keep': 0.6698, 'eval_correct_token': 0.6101, 'epoch': 20.0, 'total_flos': 425389897728000, 'step': 140}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d58dfca14564cf888a2a1f1e2abb426",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=7.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae1fa3ed055b4953b6d845ef1f8e54fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=7.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f040bbd1d2a946199500d7dfa831774c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=7.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0008653640747070312, 'learning_rate': 1.5625e-05, 'epoch': 22.857142857142858, 'total_flos': 488347602591744, 'step': 160}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7288e5407a79466ca131a1fa23121dc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluation', max=7.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 102 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 102 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 102 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 102 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 102 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 102 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 102 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:filelock:Lock 46917148054736 acquired on /home/zonghaiyao/.cache/huggingface/metrics/rouge/default/9-1-0.arrow.lock\n",
      "INFO:filelock:Lock 46917148054736 released on /home/zonghaiyao/.cache/huggingface/metrics/rouge/default/9-1-0.arrow.lock\n",
      "INFO:filelock:Lock 46917464482960 acquired on /home/zonghaiyao/.cache/huggingface/metrics/rouge/default/9-1-0.arrow.lock\n",
      "INFO:/mnt/nfs/work1/llcao/zonghaiyao/tweetNorm/datasets/src/datasets/metric.py:Removing /home/zonghaiyao/.cache/huggingface/metrics/rouge/default/9-1-0.arrow\n",
      "INFO:filelock:Lock 46917464482960 released on /home/zonghaiyao/.cache/huggingface/metrics/rouge/default/9-1-0.arrow.lock\n",
      "INFO:filelock:Lock 46917467866704 acquired on /home/zonghaiyao/.cache/huggingface/metrics/bleu/default/9-1-0.arrow.lock\n",
      "INFO:filelock:Lock 46917467866704 released on /home/zonghaiyao/.cache/huggingface/metrics/bleu/default/9-1-0.arrow.lock\n",
      "INFO:filelock:Lock 46917469176016 acquired on /home/zonghaiyao/.cache/huggingface/metrics/bleu/default/9-1-0.arrow.lock\n",
      "INFO:/mnt/nfs/work1/llcao/zonghaiyao/tweetNorm/datasets/src/datasets/metric.py:Removing /home/zonghaiyao/.cache/huggingface/metrics/bleu/default/9-1-0.arrow\n",
      "INFO:filelock:Lock 46917469176016 released on /home/zonghaiyao/.cache/huggingface/metrics/bleu/default/9-1-0.arrow.lock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3514905997685023, 'eval_rouge1_precision': 0.8122, 'eval_rouge1_recall': 0.7773, 'eval_rouge1_fmeasure': 0.7911, 'eval_rouge2_precision': 0.6905, 'eval_rouge2_recall': 0.6589, 'eval_rouge2_fmeasure': 0.6712, 'eval_rouge3_precision': 0.5795, 'eval_rouge3_recall': 0.5522, 'eval_rouge3_fmeasure': 0.5627, 'eval_rouge4_precision': 0.4889, 'eval_rouge4_recall': 0.4647, 'eval_rouge4_fmeasure': 0.4731, 'eval_rougeL_precision': 0.8104, 'eval_rougeL_recall': 0.7754, 'eval_rougeL_fmeasure': 0.7891, 'eval_rougeLsum_precision': 0.8101, 'eval_rougeLsum_recall': 0.7748, 'eval_rougeLsum_fmeasure': 0.7889, 'eval_bleu': 0.6965, 'eval_norm_correct': 56.0, 'eval_norm_wrong': 169.0, 'eval_keep_correct': 940.0, 'eval_keep_wrong': 438.0, 'eval_correct_of_all_need_norm': 0.2489, 'eval_correct_of_all_need_keep': 0.6821, 'eval_correct_token': 0.6213, 'epoch': 22.857142857142858, 'total_flos': 488347602591744, 'step': 160}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5a04ab6a4f6483e875aeac0fb2bd916",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=7.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "011fb07a56b04e68b81f1b4bd318135f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=7.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-16fac7b03511>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;31m# start training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/mnt/nfs/work1/llcao/zonghaiyao/tweetNorm/transformers/src/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, model_path, trial)\u001b[0m\n\u001b[1;32m    788\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 790\u001b[0;31m                 \u001b[0mtr_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    791\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_flos\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloating_point_ops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/nfs/work1/llcao/zonghaiyao/tweetNorm/transformers/src/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   1096\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1097\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp16\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_use_native_amp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp16\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_use_apex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mscaled_loss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tweetNorm/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tweetNorm/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    125\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#-----------------------------------------------------------------------------------\n",
    "# global record\n",
    "# record = {}\n",
    "# load metrics for validation\n",
    "# Calculate previous_metrics score of a corpus.\n",
    "def previous_metrics(pred):\n",
    "#     global record\n",
    "    pred_ids = pred.predictions\n",
    "    is_labels_need_change = pred.is_labels_need_change\n",
    "\n",
    "    def batch_convert_ids_to_tokens(sequences, **kwargs):\n",
    "        return [decoder_tokenizer.decode(seq, **kwargs) for seq in sequences]\n",
    "\n",
    "    pred_tokens = batch_convert_ids_to_tokens(pred_ids, skip_special_tokens=True)\n",
    "\n",
    "    assert len(pred_tokens) == len(is_labels_need_change)\n",
    "    norm_correct, norm_wrong, keep_correct, keep_wrong = 0.0, 0.0, 0.0, 0.0\n",
    "\n",
    "    for pred_tokens, oracle_tokens in zip(pred_tokens, is_labels_need_change):\n",
    "        pred_tokens = pred_tokens.split()\n",
    "        sent_length = len(oracle_tokens)\n",
    "\n",
    "        while len(pred_tokens) < len(oracle_tokens) : pred_tokens.append(\"<PAD>\")\n",
    "#         print(\"pred_tokens:\", pred_tokens)\n",
    "#         print(\"oracle_tokens:\", oracle_tokens)\n",
    "#         record['pred_tokens'] = pred_tokens\n",
    "#         record['oracle_tokens'] = oracle_tokens\n",
    "        \n",
    "        for i in range(sent_length):\n",
    "            pred_token = pred_tokens[i]\n",
    "            oracle_token = labels_id2word[int(oracle_tokens[i][0])]\n",
    "            token_need_change = oracle_tokens[i][1]\n",
    "            #norm-correct：需要改且改对的\n",
    "            if token_need_change == 1 and oracle_token.lower() == pred_token.lower() and oracle_token.strip():\n",
    "                norm_correct += 1\n",
    "            #norm-wrong ：需要改但没改对的\n",
    "            if token_need_change == 1 and oracle_token.lower() != pred_token.lower() and oracle_token.strip():\n",
    "                norm_wrong += 1\n",
    "            #keep-correct：不需要改且没有改的\n",
    "            if token_need_change == 0 and oracle_token.lower() == pred_token.lower() and oracle_token.strip():\n",
    "                keep_correct += 1\n",
    "            #keep-wrong ：不需要改但是改了的\n",
    "            if token_need_change == 0 and oracle_token.lower() != pred_token.lower() and oracle_token.strip():\n",
    "                keep_wrong += 1\n",
    "    \n",
    "    #所有不需要修改token的正确率\n",
    "    correct_of_all_need_keep = keep_correct / (keep_correct + keep_wrong)\n",
    "\n",
    "    results = {}\n",
    "    results[\"keep_correct\"] = keep_correct\n",
    "    results[\"keep_wrong\"] = keep_wrong\n",
    "    results[\"correct_of_all_need_keep\"] = correct_of_all_need_keep\n",
    "    \n",
    "    if TWEET_COPY_TASK == 0:\n",
    "        #所有需要修改token的正确率\n",
    "        correct_of_all_need_norm = norm_correct / (norm_correct + norm_wrong)\n",
    "        #输出正确token的正确率\n",
    "        correct_token = (norm_correct + keep_correct) / (norm_correct + norm_wrong + keep_correct + keep_wrong)\n",
    "        results[\"norm_correct\"] = norm_correct\n",
    "        results[\"norm_wrong\"] = norm_wrong\n",
    "        results[\"correct_of_all_need_norm\"] = correct_of_all_need_norm\n",
    "        results[\"correct_token\"] = correct_token\n",
    "    \n",
    "    return results\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    #Calculate previous_metrics score of a corpus\n",
    "    previous_metrics_results = previous_metrics(pred)\n",
    "    \n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "\n",
    "    # all unnecessary tokens are removed\n",
    "    pred_str = decoder_tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    labels_ids[labels_ids == -100] = decoder_tokenizer.eos_token_id\n",
    "    label_str = decoder_tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "    \n",
    "    metrics_rouge = rouge.compute(predictions=pred_str, references=label_str, rouge_types=[\"rouge1\", \"rouge2\", \"rouge3\", \"rouge4\", \"rougeL\", \"rougeLsum\"])\n",
    "\n",
    "    def batch_convert_ids_to_tokens(sequences, **kwargs):\n",
    "        return [decoder_tokenizer.convert_ids_to_tokens(seq, **kwargs) for seq in sequences]\n",
    "    \n",
    "    pred_tokens = batch_convert_ids_to_tokens(pred_ids, skip_special_tokens=True)\n",
    "    \n",
    "    def batch_convert_ids_to_tokens(sequences, **kwargs):\n",
    "        return [[decoder_tokenizer.convert_ids_to_tokens(seq, **kwargs)] for seq in sequences]\n",
    "    \n",
    "    label_tokens = batch_convert_ids_to_tokens(labels_ids, skip_special_tokens=True)\n",
    "    \n",
    "    metrics_bleu = bleu.compute(predictions=pred_tokens, references=label_tokens)\n",
    "    \n",
    "    if TWEET_COPY_TASK == 1:\n",
    "        return {\n",
    "            \"rouge1_precision\": round(metrics_rouge['rouge1'].mid.precision, 4),\n",
    "            \"rouge1_recall\": round(metrics_rouge['rouge1'].mid.recall, 4),\n",
    "            \"rouge1_fmeasure\": round(metrics_rouge['rouge1'].mid.fmeasure, 4),\n",
    "            \"rouge2_precision\": round(metrics_rouge['rouge2'].mid.precision, 4),\n",
    "            \"rouge2_recall\": round(metrics_rouge['rouge2'].mid.recall, 4),\n",
    "            \"rouge2_fmeasure\": round(metrics_rouge['rouge2'].mid.fmeasure, 4),\n",
    "            \"rouge3_precision\": round(metrics_rouge['rouge3'].mid.precision, 4),\n",
    "            \"rouge3_recall\": round(metrics_rouge['rouge3'].mid.recall, 4),\n",
    "            \"rouge3_fmeasure\": round(metrics_rouge['rouge3'].mid.fmeasure, 4),\n",
    "            \"rouge4_precision\": round(metrics_rouge['rouge4'].mid.precision, 4),\n",
    "            \"rouge4_recall\": round(metrics_rouge['rouge4'].mid.recall, 4),\n",
    "            \"rouge4_fmeasure\": round(metrics_rouge['rouge4'].mid.fmeasure, 4),\n",
    "            \"rougeL_precision\": round(metrics_rouge['rougeL'].mid.precision, 4),\n",
    "            \"rougeL_recall\": round(metrics_rouge['rougeL'].mid.recall, 4),\n",
    "            \"rougeL_fmeasure\": round(metrics_rouge['rougeL'].mid.fmeasure, 4),\n",
    "            \"rougeLsum_precision\": round(metrics_rouge['rougeLsum'].mid.precision, 4),\n",
    "            \"rougeLsum_recall\": round(metrics_rouge['rougeLsum'].mid.recall, 4),\n",
    "            \"rougeLsum_fmeasure\": round(metrics_rouge['rougeLsum'].mid.fmeasure, 4),\n",
    "            \"bleu\": round(metrics_bleu['bleu'], 4),\n",
    "            \"keep_correct\": round(previous_metrics_results['keep_correct'], 4),\n",
    "            \"keep_wrong\": round(previous_metrics_results['keep_wrong'], 4),\n",
    "            \"correct_of_all_need_keep\": round(previous_metrics_results['correct_of_all_need_keep'], 4),\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            \"rouge1_precision\": round(metrics_rouge['rouge1'].mid.precision, 4),\n",
    "            \"rouge1_recall\": round(metrics_rouge['rouge1'].mid.recall, 4),\n",
    "            \"rouge1_fmeasure\": round(metrics_rouge['rouge1'].mid.fmeasure, 4),\n",
    "            \"rouge2_precision\": round(metrics_rouge['rouge2'].mid.precision, 4),\n",
    "            \"rouge2_recall\": round(metrics_rouge['rouge2'].mid.recall, 4),\n",
    "            \"rouge2_fmeasure\": round(metrics_rouge['rouge2'].mid.fmeasure, 4),\n",
    "            \"rouge3_precision\": round(metrics_rouge['rouge3'].mid.precision, 4),\n",
    "            \"rouge3_recall\": round(metrics_rouge['rouge3'].mid.recall, 4),\n",
    "            \"rouge3_fmeasure\": round(metrics_rouge['rouge3'].mid.fmeasure, 4),\n",
    "            \"rouge4_precision\": round(metrics_rouge['rouge4'].mid.precision, 4),\n",
    "            \"rouge4_recall\": round(metrics_rouge['rouge4'].mid.recall, 4),\n",
    "            \"rouge4_fmeasure\": round(metrics_rouge['rouge4'].mid.fmeasure, 4),\n",
    "            \"rougeL_precision\": round(metrics_rouge['rougeL'].mid.precision, 4),\n",
    "            \"rougeL_recall\": round(metrics_rouge['rougeL'].mid.recall, 4),\n",
    "            \"rougeL_fmeasure\": round(metrics_rouge['rougeL'].mid.fmeasure, 4),\n",
    "            \"rougeLsum_precision\": round(metrics_rouge['rougeLsum'].mid.precision, 4),\n",
    "            \"rougeLsum_recall\": round(metrics_rouge['rougeLsum'].mid.recall, 4),\n",
    "            \"rougeLsum_fmeasure\": round(metrics_rouge['rougeLsum'].mid.fmeasure, 4),\n",
    "            \"bleu\": round(metrics_bleu['bleu'], 4),\n",
    "            \"norm_correct\": round(previous_metrics_results['norm_correct'], 4),\n",
    "            \"norm_wrong\": round(previous_metrics_results['norm_wrong'], 4),\n",
    "            \"keep_correct\": round(previous_metrics_results['keep_correct'], 4),\n",
    "            \"keep_wrong\": round(previous_metrics_results['keep_wrong'], 4),\n",
    "            \"correct_of_all_need_norm\": round(previous_metrics_results['correct_of_all_need_norm'], 4),\n",
    "            \"correct_of_all_need_keep\": round(previous_metrics_results['correct_of_all_need_keep'], 4),\n",
    "            \"correct_token\": round(previous_metrics_results['correct_token'], 4),\n",
    "        }\n",
    "        \n",
    "#----------------------------------------------------------------------------------------\n",
    "# begin train\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    predict_from_generate=True,\n",
    "    evaluate_during_training=True,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    logging_steps=20,\n",
    "    save_steps=100,\n",
    "    eval_steps=20,\n",
    "    overwrite_output_dir=True,\n",
    "    warmup_steps=50,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=30,\n",
    "\n",
    "    \n",
    "    seed=SEED,\n",
    "    fp16=True,\n",
    "    run_name=RUN_NAME,\n",
    ")\n",
    "\n",
    "\n",
    "# instantiate trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    my_trainer = True\n",
    ")\n",
    "\n",
    "\n",
    "# start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate previous_metrics score of a corpus.\n",
    "# def previous_metrics(pred):\n",
    "#     pred_ids = pred.predictions\n",
    "#     is_labels_need_change = pred.is_labels_need_change\n",
    "\n",
    "#     def batch_convert_ids_to_tokens(sequences, **kwargs):\n",
    "#         return [decoder_tokenizer.decode(seq, **kwargs) for seq in sequences]\n",
    "\n",
    "#     pred_tokens = batch_convert_ids_to_tokens(pred_ids, skip_special_tokens=True)\n",
    "\n",
    "#     assert len(pred_tokens) == len(is_labels_need_change)\n",
    "#     norm_correct, norm_wrong, keep_correct, keep_wrong = 0.0, 0.0, 0.0, 0.0\n",
    "\n",
    "#     for pred_tokens, oracle_tokens in zip(pred_tokens, is_labels_need_change):\n",
    "#         sent_length = len(oracle_tokens)\n",
    "#         while len(pred_tokens) < len(oracle_tokens) : pred_tokens.append(\"<PAD>\")\n",
    "#         for i in range(sent_length):\n",
    "#             pred_token = pred_tokens[i]\n",
    "#             oracle_token = labels_id2word[int(oracle_tokens[i][0])]\n",
    "#             token_need_change = oracle_tokens[i][1]\n",
    "#             #norm-correct：需要改且改对的\n",
    "#             if token_need_change == 1 and oracle_token.lower() == pred_token.lower() and oracle_token.strip():\n",
    "#                 norm_correct += 1\n",
    "#             #norm-wrong ：需要改但没改对的\n",
    "#             if token_need_change == 1 and oracle_token.lower() != pred_token.lower() and oracle_token.strip():\n",
    "#                 norm_wrong += 1\n",
    "#             #keep-correct：不需要改且没有改的\n",
    "#             if token_need_change == 0 and oracle_token.lower() == pred_token.lower() and oracle_token.strip():\n",
    "#                 keep_correct += 1\n",
    "#             #keep-wrong ：不需要改但是改了的\n",
    "#             if token_need_change == 0 and oracle_token.lower() != pred_token.lower() and oracle_token.strip():\n",
    "#                 keep_wrong += 1\n",
    "#     #所有需要修改token的正确率\n",
    "#     correct_of_all_need_norm = norm_correct / (norm_correct + norm_wrong)\n",
    "#     #所有不需要修改token的正确率\n",
    "#     correct_of_all_need_keep = keep_correct / (keep_correct + keep_wrong)\n",
    "#     #输出正确token的正确率\n",
    "#     correct_token = (norm_correct + keep_correct) / (norm_correct + norm_wrong + keep_correct + keep_wrong)\n",
    "\n",
    "#     results = {}\n",
    "#     results[\"norm_correct\"] = norm_correct\n",
    "#     results[\"norm_wrong\"] = norm_wrong\n",
    "#     results[\"keep_correct\"] = keep_correct\n",
    "#     results[\"keep_wrong\"] = keep_wrong\n",
    "#     results[\"correct_of_all_need_norm\"] = correct_of_all_need_norm\n",
    "#     results[\"correct_of_all_need_keep\"] = correct_of_all_need_keep\n",
    "#     results[\"correct_token\"] = correct_token\n",
    "#     return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'norm_correct': 0.0,\n",
       " 'norm_wrong': 107.0,\n",
       " 'keep_correct': 20.0,\n",
       " 'keep_wrong': 1471.0,\n",
       " 'correct_of_all_need_norm': 0.0,\n",
       " 'correct_of_all_need_keep': 0.01341381623071764,\n",
       " 'correct_token': 0.012515644555694618}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# previous_metrics(pred_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load train and validation data\n",
    "# train_df = pd.read_json(\"WNUT2015_dataset/train_data.json\", orient=\"records\")\n",
    "# val_df = pd.read_json(\"WNUT2015_dataset/test_truth.json\", orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41 41 41 41\n",
      "4 4 4 4\n"
     ]
    }
   ],
   "source": [
    "# get_len = lambda x : len(x)\n",
    "\n",
    "# train_df['input_length'] = train_df['input'].apply(get_len)\n",
    "# train_df['output_length'] = train_df['output'].apply(get_len)\n",
    "# val_df['input_length'] = val_df['input'].apply(get_len)\n",
    "# val_df['output_length'] = val_df['output'].apply(get_len)\n",
    "\n",
    "# print(max(train_df['input_length']), max(train_df['output_length']), max(val_df['input_length']), max(val_df['output_length']))\n",
    "# print(min(train_df['input_length']), min(train_df['output_length']), min(val_df['input_length']), min(val_df['output_length']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
