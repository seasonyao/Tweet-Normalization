{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pre-pocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tweetNormalizer import normalizeTweet\n",
    "import re\n",
    "\n",
    "# load train and validation data\n",
    "train_dataset = pd.read_json(\"WNUT2015_dataset/train_data.json\", orient=\"records\")\n",
    "val_dataset = pd.read_json(\"WNUT2015_dataset/test_truth.json\", orient=\"records\")\n",
    "\n",
    "train_dataset = train_dataset[:100]\n",
    "val_dataset = val_dataset[:5]\n",
    "\n",
    "\n",
    "make_sentence = lambda x : normalizeTweet(\" \".join(x))\n",
    "\n",
    "train_dataset['input_sentence'] = train_dataset['input'].apply(make_sentence)\n",
    "train_dataset['output_sentence'] = train_dataset['output'].apply(make_sentence)\n",
    "val_dataset['input_sentence'] = val_dataset['input'].apply(make_sentence)\n",
    "val_dataset['output_sentence'] = val_dataset['output'].apply(make_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> RT @USER : [ HQ ] 14@@ 05@@ 26 Xi@@ umin , Luhan @USER MBC Idol Fut@@ sal Championship ( cr . shade of the bloom ) HTTPURL </s>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertweetTokenizer, AutoTokenizer\n",
    "from tweetNormalizer import normalizeTweet\n",
    "\n",
    "#line = \"SC has first two presumptive cases of coronavirus, DHEC confirms https://postandcourier.com/health/covid19/sc-has-first-two-presumptive-cases-of-coronavirus-dhec-confirms/article_bddfe4ae-5fd3-11ea-9ce4-5f495366cee6.html?utm_medium=social&utm_source=twitter&utm_campaign=user-share… via @postandcourier\"\n",
    "line = \"RT @EXOffical : [ HQ ] 140526 Xiumin , Luhan @ MBC Idol Futsal Championship ( cr . shade of the bloom ) http://t.co/ToBKl76SzP\"\n",
    "\n",
    "\n",
    "tokenizer = BertweetTokenizer.from_pretrained(\"vinai/bertweet-base\")\n",
    "input_ids = torch.tensor([tokenizer.encode(normalizeTweet(line))])\n",
    "print(' '.join(tokenizer.convert_ids_to_tokens(input_ids[0], skip_special_tokens=False)))\n",
    "\n",
    "# tokenizer = BertweetTokenizer.from_pretrained(\"vinai/bertweet-base\", normalization=True)\n",
    "# input_ids = torch.tensor([tokenizer.encode(line)])\n",
    "# print(' '.join(tokenizer.convert_ids_to_tokens(input_ids[0], skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64000\n",
      "0 64000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bos_token': '<s>',\n",
       " 'eos_token': '</s>',\n",
       " 'unk_token': '<unk>',\n",
       " 'sep_token': '</s>',\n",
       " 'pad_token': '<pad>',\n",
       " 'cls_token': '<s>',\n",
       " 'mask_token': '<mask>',\n",
       " 'additional_special_tokens': \"['@USER', 'HTTPURL']\"}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", normalization=True)\n",
    "special_tokens_dict = {'additional_special_tokens': ['@USER','HTTPURL']}\n",
    "print(tokenizer.vocab_size)\n",
    "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "print(num_added_toks, tokenizer.vocab_size)\n",
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30522\n",
      "2 30522\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'unk_token': '[UNK]',\n",
       " 'sep_token': '[SEP]',\n",
       " 'pad_token': '[PAD]',\n",
       " 'cls_token': '[CLS]',\n",
       " 'mask_token': '[MASK]',\n",
       " 'additional_special_tokens': \"['@USER', 'HTTPURL']\"}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "special_tokens_dict = {'additional_special_tokens': ['@USER','HTTPURL']}\n",
    "print(tokenizer.vocab_size)\n",
    "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "print(num_added_toks, tokenizer.vocab_size)\n",
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@USER', 'HTTPURL']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.additional_special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50257\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bos_token': '<|endoftext|>',\n",
       " 'eos_token': '<|endoftext|>',\n",
       " 'unk_token': '<|endoftext|>',\n",
       " 'additional_special_tokens': \"['@USER', 'HTTPURL']\"}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "special_tokens_dict = {'additional_special_tokens': ['@USER','HTTPURL']}\n",
    "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "print(tokenizer.vocab_size)\n",
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50265\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bos_token': '<s>',\n",
       " 'eos_token': '</s>',\n",
       " 'unk_token': '<unk>',\n",
       " 'sep_token': '</s>',\n",
       " 'pad_token': '<pad>',\n",
       " 'cls_token': '<s>',\n",
       " 'mask_token': '<mask>',\n",
       " 'additional_special_tokens': \"['@USER', 'HTTPURL']\"}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "special_tokens_dict = {'additional_special_tokens': ['@USER','HTTPURL']}\n",
    "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "print(tokenizer.vocab_size)\n",
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sc has first two presumptive cases of coronavirus, dhec confirms HTTPURL... via @USER'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line = \"SC has first two presumptive cases of coronavirus, DHEC confirms https://postandcourier.com/health/covid19/sc-has-first-two-presumptive-cases-of-coronavirus-dhec-confirms/article_bddfe4ae-5fd3-11ea-9ce4-5f495366cee6.html?utm_medium=social&utm_source=twitter&utm_campaign=user-share… via @postandcourier\"\n",
    "\n",
    "ENCODER = \"bert-base-uncased\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(ENCODER)\n",
    "\n",
    "if ENCODER==\"bert-base-uncased\":\n",
    "    # CLS token will work as BOS token, SEP token will work as EOS token\n",
    "    tokenizer.bos_token = tokenizer.cls_token\n",
    "    tokenizer.eos_token = tokenizer.sep_token\n",
    "\n",
    "special_tokens_dict = {'additional_special_tokens': ['@USER','HTTPURL']}\n",
    "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "input_ids = tokenizer.encode(normalizeTweet(line))\n",
    "\n",
    "for rm_id in [tokenizer.bos_token_id, tokenizer.eos_token_id, tokenizer.pad_token_id]:\n",
    "    if rm_id in input_ids:\n",
    "        input_ids.remove(rm_id)\n",
    "\n",
    "input_ids = torch.tensor([input_ids])\n",
    "\n",
    "input_sent = tokenizer.decode(input_ids[0], skip_special_tokens=False)\n",
    "input_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertLMHeadModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertLMHeadModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.self.query.bias', 'bert.encoder.layer.1.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.self.value.bias', 'bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.3.crossattention.self.query.weight', 'bert.encoder.layer.3.crossattention.self.query.bias', 'bert.encoder.layer.3.crossattention.self.key.weight', 'bert.encoder.layer.3.crossattention.self.key.bias', 'bert.encoder.layer.3.crossattention.self.value.weight', 'bert.encoder.layer.3.crossattention.self.value.bias', 'bert.encoder.layer.3.crossattention.output.dense.weight', 'bert.encoder.layer.3.crossattention.output.dense.bias', 'bert.encoder.layer.3.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.3.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.5.crossattention.self.query.weight', 'bert.encoder.layer.5.crossattention.self.query.bias', 'bert.encoder.layer.5.crossattention.self.key.weight', 'bert.encoder.layer.5.crossattention.self.key.bias', 'bert.encoder.layer.5.crossattention.self.value.weight', 'bert.encoder.layer.5.crossattention.self.value.bias', 'bert.encoder.layer.5.crossattention.output.dense.weight', 'bert.encoder.layer.5.crossattention.output.dense.bias', 'bert.encoder.layer.5.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.5.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.7.crossattention.self.query.weight', 'bert.encoder.layer.7.crossattention.self.query.bias', 'bert.encoder.layer.7.crossattention.self.key.weight', 'bert.encoder.layer.7.crossattention.self.key.bias', 'bert.encoder.layer.7.crossattention.self.value.weight', 'bert.encoder.layer.7.crossattention.self.value.bias', 'bert.encoder.layer.7.crossattention.output.dense.weight', 'bert.encoder.layer.7.crossattention.output.dense.bias', 'bert.encoder.layer.7.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.7.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.9.crossattention.self.query.weight', 'bert.encoder.layer.9.crossattention.self.query.bias', 'bert.encoder.layer.9.crossattention.self.key.weight', 'bert.encoder.layer.9.crossattention.self.key.bias', 'bert.encoder.layer.9.crossattention.self.value.weight', 'bert.encoder.layer.9.crossattention.self.value.bias', 'bert.encoder.layer.9.crossattention.output.dense.weight', 'bert.encoder.layer.9.crossattention.output.dense.bias', 'bert.encoder.layer.9.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.9.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.11.crossattention.self.query.weight', 'bert.encoder.layer.11.crossattention.self.query.bias', 'bert.encoder.layer.11.crossattention.self.key.weight', 'bert.encoder.layer.11.crossattention.self.key.bias', 'bert.encoder.layer.11.crossattention.self.value.weight', 'bert.encoder.layer.11.crossattention.self.value.bias', 'bert.encoder.layer.11.crossattention.output.dense.weight', 'bert.encoder.layer.11.crossattention.output.dense.bias', 'bert.encoder.layer.11.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.11.crossattention.output.LayerNorm.bias', 'cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# set EncoderDecoderModel\n",
    "model = EncoderDecoderModel.from_encoder_decoder_pretrained(\"bert-base-uncased\", \"bert-base-uncased\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train token classification for whether it need to be changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForTokenClassification\n",
    "import torch\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForTokenClassification.from_pretrained('bert-base-uncased', return_dict=True)\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "labels = torch.tensor([1] * inputs[\"input_ids\"].size(1)).unsqueeze(0)  # Batch size 1\n",
    "outputs = model(**inputs, labels=labels)\n",
    "# loss = outputs.loss\n",
    "# logits = outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TokenClassifierOutput(loss=tensor(0.8729, grad_fn=<NllLossBackward>), logits=tensor([[[ 0.0837, -0.0871],\n",
       "         [ 0.0103,  0.0035],\n",
       "         [ 0.3324, -0.1649],\n",
       "         [ 0.1886,  0.1879],\n",
       "         [ 0.5739, -0.0223],\n",
       "         [ 0.2863, -0.1605],\n",
       "         [ 0.5136, -0.2981],\n",
       "         [-0.2135, -0.1934]]], grad_fn=<AddBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils_ner'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-dcfd04fed309>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mset_seed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m )\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils_ner\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSplit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTokenClassificationDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTokenClassificationTask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils_ner'"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from dataclasses import dataclass, field\n",
    "from importlib import import_module\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "from seqeval.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from torch import nn\n",
    "\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoTokenizer,\n",
    "    EvalPrediction,\n",
    "    HfArgumentParser,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "from utils_ner import Split, TokenClassificationDataset, TokenClassificationTask\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n",
    "    \"\"\"\n",
    "\n",
    "    model_name_or_path: str = field(\n",
    "        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n",
    "    )\n",
    "    config_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n",
    "    )\n",
    "    task_type: Optional[str] = field(\n",
    "        default=\"NER\", metadata={\"help\": \"Task type to fine tune in training (e.g. NER, POS, etc)\"}\n",
    "    )\n",
    "    tokenizer_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n",
    "    )\n",
    "    use_fast: bool = field(default=False, metadata={\"help\": \"Set this flag to use fast tokenization.\"})\n",
    "    # If you want to tweak more attributes on your tokenizer, you should do it in a distinct script,\n",
    "    # or just modify its tokenizer_config.json.\n",
    "    cache_dir: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Where do you want to store the pretrained models downloaded from s3\"}\n",
    "    )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataTrainingArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
    "    \"\"\"\n",
    "\n",
    "    data_dir: str = field(\n",
    "        metadata={\"help\": \"The input data dir. Should contain the .txt files for a CoNLL-2003-formatted task.\"}\n",
    "    )\n",
    "    labels: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"Path to a file containing all labels. If not specified, CoNLL-2003 labels are used.\"},\n",
    "    )\n",
    "    max_seq_length: int = field(\n",
    "        default=128,\n",
    "        metadata={\n",
    "            \"help\": \"The maximum total input sequence length after tokenization. Sequences longer \"\n",
    "            \"than this will be truncated, sequences shorter will be padded.\"\n",
    "        },\n",
    "    )\n",
    "    overwrite_cache: bool = field(\n",
    "        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n",
    "    )\n",
    "\n",
    "\n",
    "def main():\n",
    "    # See all possible arguments in src/transformers/training_args.py\n",
    "    # or by passing the --help flag to this script.\n",
    "    # We now keep distinct sets of args, for a cleaner separation of concerns.\n",
    "\n",
    "    model_args = ModelArguments(model_name_or_path='bert-base-multilingual-cased', \n",
    "                                config_name=None,\n",
    "                                task_type='NER', \n",
    "                                tokenizer_name=None,\n",
    "                                use_fast=False,\n",
    "                                cache_dir=None)\n",
    "    data_args = DataTrainingArguments(data_dir='.', \n",
    "                                      labels='./labels.txt', \n",
    "                                      max_seq_length=128,\n",
    "                                      overwrite_cache=False)\n",
    "    training_args = TrainingArguments(output_dir='germeval-model',\n",
    "                                      overwrite_output_dir=False, \n",
    "                                      do_train=True,\n",
    "                                      do_eval=True, \n",
    "                                      do_predict=True,\n",
    "                                      evaluate_during_training=False, \n",
    "                                      prediction_loss_only=False,\n",
    "                                      per_device_train_batch_size=8, \n",
    "                                      per_device_eval_batch_size=8, \n",
    "                                      per_gpu_train_batch_size=32, \n",
    "                                      per_gpu_eval_batch_size=None, \n",
    "                                      gradient_accumulation_steps=1, \n",
    "                                      predict_from_generate=False, \n",
    "                                      learning_rate=5e-05, \n",
    "                                      weight_decay=0.0,\n",
    "                                      adam_beta1=0.9,\n",
    "                                      adam_beta2=0.999,\n",
    "                                      adam_epsilon=1e-08,\n",
    "                                      max_grad_norm=1.0,\n",
    "                                      num_train_epochs=3.0, \n",
    "                                      max_steps=-1,\n",
    "                                      warmup_steps=0,\n",
    "                                      logging_dir='runs/Oct07_14-28-16_node002',\n",
    "                                      logging_first_step=False, \n",
    "                                      logging_steps=500,\n",
    "                                      save_steps=750, \n",
    "                                      save_total_limit=None,\n",
    "                                      no_cuda=False,\n",
    "                                      seed=1, \n",
    "                                      fp16=False, \n",
    "                                      fp16_opt_level='O1', \n",
    "                                      local_rank=-1, \n",
    "                                      tpu_num_cores=None,\n",
    "                                      tpu_metrics_debug=False,\n",
    "                                      debug=False,\n",
    "                                      dataloader_drop_last=False, \n",
    "                                      eval_steps=1000,\n",
    "                                      past_index=-1,\n",
    "                                      run_name=None, \n",
    "                                      disable_tqdm=False,\n",
    "                                      remove_unused_columns=True,\n",
    "                                      label_names=None)\n",
    "\n",
    "\n",
    "    if (\n",
    "        os.path.exists(training_args.output_dir)\n",
    "        and os.listdir(training_args.output_dir)\n",
    "        and training_args.do_train\n",
    "        and not training_args.overwrite_output_dir\n",
    "    ):\n",
    "        raise ValueError(\n",
    "            f\"Output directory ({training_args.output_dir}) already exists and is not empty. Use --overwrite_output_dir to overcome.\"\n",
    "        )\n",
    "\n",
    "    module = import_module(\"tasks\")\n",
    "    try:\n",
    "        token_classification_task_clazz = getattr(module, model_args.task_type)\n",
    "        token_classification_task: TokenClassificationTask = token_classification_task_clazz()\n",
    "    except AttributeError:\n",
    "        raise ValueError(\n",
    "            f\"Task {model_args.task_type} needs to be defined as a TokenClassificationTask subclass in {module}. \"\n",
    "            f\"Available tasks classes are: {TokenClassificationTask.__subclasses__()}\"\n",
    "        )\n",
    "\n",
    "    # Setup logging\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO if training_args.local_rank in [-1, 0] else logging.WARN,\n",
    "    )\n",
    "    logger.warning(\n",
    "        \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n",
    "        training_args.local_rank,\n",
    "        training_args.device,\n",
    "        training_args.n_gpu,\n",
    "        bool(training_args.local_rank != -1),\n",
    "        training_args.fp16,\n",
    "    )\n",
    "    logger.info(\"Training/evaluation parameters %s\", training_args)\n",
    "\n",
    "    # Set seed\n",
    "    set_seed(training_args.seed)\n",
    "\n",
    "    # Prepare CONLL-2003 task\n",
    "    labels = token_classification_task.get_labels(data_args.labels)\n",
    "    label_map: Dict[int, str] = {i: label for i, label in enumerate(labels)}\n",
    "    num_labels = len(labels)\n",
    "\n",
    "    # Load pretrained model and tokenizer\n",
    "    #\n",
    "    # Distributed training:\n",
    "    # The .from_pretrained methods guarantee that only one local process can concurrently\n",
    "    # download model & vocab.\n",
    "\n",
    "    config = AutoConfig.from_pretrained(\n",
    "        model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n",
    "        num_labels=num_labels,\n",
    "        id2label=label_map,\n",
    "        label2id={label: i for i, label in enumerate(labels)},\n",
    "        cache_dir=model_args.cache_dir,\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,\n",
    "        cache_dir=model_args.cache_dir,\n",
    "        use_fast=model_args.use_fast,\n",
    "    )\n",
    "    model = AutoModelForTokenClassification.from_pretrained(\n",
    "        model_args.model_name_or_path,\n",
    "        from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n",
    "        config=config,\n",
    "        cache_dir=model_args.cache_dir,\n",
    "    )\n",
    "\n",
    "    # Get datasets\n",
    "    train_dataset = (\n",
    "        TokenClassificationDataset(\n",
    "            token_classification_task=token_classification_task,\n",
    "            data_dir=data_args.data_dir,\n",
    "            tokenizer=tokenizer,\n",
    "            labels=labels,\n",
    "            model_type=config.model_type,\n",
    "            max_seq_length=data_args.max_seq_length,\n",
    "            overwrite_cache=data_args.overwrite_cache,\n",
    "            mode=Split.train,\n",
    "        )\n",
    "        if training_args.do_train\n",
    "        else None\n",
    "    )\n",
    "    eval_dataset = (\n",
    "        TokenClassificationDataset(\n",
    "            token_classification_task=token_classification_task,\n",
    "            data_dir=data_args.data_dir,\n",
    "            tokenizer=tokenizer,\n",
    "            labels=labels,\n",
    "            model_type=config.model_type,\n",
    "            max_seq_length=data_args.max_seq_length,\n",
    "            overwrite_cache=data_args.overwrite_cache,\n",
    "            mode=Split.dev,\n",
    "        )\n",
    "        if training_args.do_eval\n",
    "        else None\n",
    "    )\n",
    "\n",
    "    def align_predictions(predictions: np.ndarray, label_ids: np.ndarray) -> Tuple[List[int], List[int]]:\n",
    "        preds = np.argmax(predictions, axis=2)\n",
    "\n",
    "        batch_size, seq_len = preds.shape\n",
    "\n",
    "        out_label_list = [[] for _ in range(batch_size)]\n",
    "        preds_list = [[] for _ in range(batch_size)]\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            for j in range(seq_len):\n",
    "                if label_ids[i, j] != nn.CrossEntropyLoss().ignore_index:\n",
    "                    out_label_list[i].append(label_map[label_ids[i][j]])\n",
    "                    preds_list[i].append(label_map[preds[i][j]])\n",
    "\n",
    "        return preds_list, out_label_list\n",
    "\n",
    "    def compute_metrics(p: EvalPrediction) -> Dict:\n",
    "        preds_list, out_label_list = align_predictions(p.predictions, p.label_ids)\n",
    "        return {\n",
    "            \"accuracy_score\": accuracy_score(out_label_list, preds_list),\n",
    "            \"precision\": precision_score(out_label_list, preds_list),\n",
    "            \"recall\": recall_score(out_label_list, preds_list),\n",
    "            \"f1\": f1_score(out_label_list, preds_list),\n",
    "        }\n",
    "\n",
    "    # Initialize our Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    # Training\n",
    "    if training_args.do_train:\n",
    "        trainer.train(\n",
    "            model_path=model_args.model_name_or_path if os.path.isdir(model_args.model_name_or_path) else None\n",
    "        )\n",
    "        trainer.save_model()\n",
    "        # For convenience, we also re-save the tokenizer to the same directory,\n",
    "        # so that you can share your model easily on huggingface.co/models =)\n",
    "        if trainer.is_world_master():\n",
    "            tokenizer.save_pretrained(training_args.output_dir)\n",
    "\n",
    "    # Evaluation\n",
    "    results = {}\n",
    "    if training_args.do_eval:\n",
    "        logger.info(\"*** Evaluate ***\")\n",
    "\n",
    "        result = trainer.evaluate()\n",
    "\n",
    "        output_eval_file = os.path.join(training_args.output_dir, \"eval_results.txt\")\n",
    "        if trainer.is_world_master():\n",
    "            with open(output_eval_file, \"w\") as writer:\n",
    "                logger.info(\"***** Eval results *****\")\n",
    "                for key, value in result.items():\n",
    "                    logger.info(\"  %s = %s\", key, value)\n",
    "                    writer.write(\"%s = %s\\n\" % (key, value))\n",
    "\n",
    "            results.update(result)\n",
    "\n",
    "    # Predict\n",
    "    if training_args.do_predict:\n",
    "        test_dataset = TokenClassificationDataset(\n",
    "            token_classification_task=token_classification_task,\n",
    "            data_dir=data_args.data_dir,\n",
    "            tokenizer=tokenizer,\n",
    "            labels=labels,\n",
    "            model_type=config.model_type,\n",
    "            max_seq_length=data_args.max_seq_length,\n",
    "            overwrite_cache=data_args.overwrite_cache,\n",
    "            mode=Split.test,\n",
    "        )\n",
    "\n",
    "        predictions, label_ids, metrics = trainer.predict(test_dataset)\n",
    "        preds_list, _ = align_predictions(predictions, label_ids)\n",
    "\n",
    "        output_test_results_file = os.path.join(training_args.output_dir, \"test_results.txt\")\n",
    "        if trainer.is_world_master():\n",
    "            with open(output_test_results_file, \"w\") as writer:\n",
    "                for key, value in metrics.items():\n",
    "                    logger.info(\"  %s = %s\", key, value)\n",
    "                    writer.write(\"%s = %s\\n\" % (key, value))\n",
    "\n",
    "        # Save predictions\n",
    "        output_test_predictions_file = os.path.join(training_args.output_dir, \"test_predictions.txt\")\n",
    "        if trainer.is_world_master():\n",
    "            with open(output_test_predictions_file, \"w\") as writer:\n",
    "                with open(os.path.join(data_args.data_dir, \"test.txt\"), \"r\") as f:\n",
    "                    token_classification_task.write_predictions_to_file(writer, f, preds_list)\n",
    "\n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# char-based model --> too slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EncoderDecoderModel, Trainer, TrainingArguments, AutoTokenizer, set_seed\n",
    "import torch\n",
    "\n",
    "# Encoding\n",
    "def encode(list_of_strings, pad_token_id=0):\n",
    "    max_length = max([len(string) for string in list_of_strings])\n",
    "\n",
    "    # create emtpy tensors\n",
    "    attention_masks = torch.zeros((len(list_of_strings), max_length), dtype=torch.long)\n",
    "    input_ids = torch.full((len(list_of_strings), max_length), pad_token_id, dtype=torch.long)\n",
    "\n",
    "    for idx, string in enumerate(list_of_strings):\n",
    "        # make sure string is in byte format\n",
    "        if not isinstance(string, bytes):\n",
    "            string = str.encode(string)\n",
    "\n",
    "        input_ids[idx, :len(string)] = torch.tensor([x + 2 for x in string])\n",
    "        attention_masks[idx, :len(string)] = 1\n",
    "\n",
    "    return input_ids, attention_masks\n",
    "    \n",
    "# Decoding\n",
    "def decode(outputs_ids):\n",
    "    decoded_outputs = []\n",
    "    for output_ids in outputs_ids.tolist():\n",
    "        # transform id back to char IDs < 2 are simply transformed to \"\"\n",
    "        decoded_outputs.append(\"\".join([chr(x - 2) if x > 1 else \"\" for x in output_ids]))\n",
    "    return decoded_outputs\n",
    "\n",
    "from transformers import ReformerModelWithLMHead\n",
    "\n",
    "model = ReformerModelWithLMHead.from_pretrained(\"google/reformer-enwik8\")\n",
    "encoded, attention_masks = encode([\"In 1965, Brooks left IBM to found the Department of\"])\n",
    "decode(model.generate(encoded, do_sample=True, max_length=150))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 看微信推送增加规则，比如增加一个sent2，从之前的error找到最典型的无法处理的情况，比如looooooooove，合成sent2送给model\n",
    "2. 查看hs2s以及monoise用我们所有metrics的结果，看看是不是可比的\n",
    "3. 完成tweet-copy任务，可以作为Intermediate fine-tuning任务\n",
    "3. 每次eval输出20个例子看看结果\n",
    "----------------------------------------------\n",
    "1. 训练token cls任务.假设3能全对训练增加align-embed过后的model.如果3和4结果都合理，考虑3、4结合到一起的流程.或者改造成类似bert预训练一样的两个loss结合的任务\n",
    "2. figure out为什么bert2bert>>others\n",
    "3. 在翻译后的结果和翻译之前跑某个sentiment analysis任务\n",
    "4. 关于新metrics\n",
    "5. 预约周五讨论和下周二讨论\n",
    "---------------------------------------------\n",
    "multi-task and Intermediate fine-tuning with token cls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:filelock:Lock 46917127741328 acquired on /home/zonghaiyao/.cache/huggingface/datasets/dcd4134ec0ad23f318793c6f8b77745d97efebf4b194bcb3c1ce90f867bec0cc.a89f1fa0750909f2d149b1ecabc808fb66cb865c94bb8bbb135c55deb50da2d7.py.lock\n",
      "INFO:filelock:Lock 46917127741328 released on /home/zonghaiyao/.cache/huggingface/datasets/dcd4134ec0ad23f318793c6f8b77745d97efebf4b194bcb3c1ce90f867bec0cc.a89f1fa0750909f2d149b1ecabc808fb66cb865c94bb8bbb135c55deb50da2d7.py.lock\n",
      "INFO:filelock:Lock 46917127905040 acquired on /home/zonghaiyao/.cache/huggingface/datasets/c7db30bf448719bd2c2ee7c233832963ab2e0b85e984dda4f577016390fa0e85.7927df63b30f94ac549ad2d2e3c61c5089402aacb0ab0478007e0abfe3431378.py.lock\n",
      "INFO:filelock:Lock 46917127905040 released on /home/zonghaiyao/.cache/huggingface/datasets/c7db30bf448719bd2c2ee7c233832963ab2e0b85e984dda4f577016390fa0e85.7927df63b30f94ac549ad2d2e3c61c5089402aacb0ab0478007e0abfe3431378.py.lock\n",
      "Some weights of BertModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.embeddings.token_align_embeddings.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertLMHeadModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertLMHeadModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.embeddings.token_align_embeddings.weight', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.self.query.bias', 'bert.encoder.layer.1.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.self.value.bias', 'bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.3.crossattention.self.query.weight', 'bert.encoder.layer.3.crossattention.self.query.bias', 'bert.encoder.layer.3.crossattention.self.key.weight', 'bert.encoder.layer.3.crossattention.self.key.bias', 'bert.encoder.layer.3.crossattention.self.value.weight', 'bert.encoder.layer.3.crossattention.self.value.bias', 'bert.encoder.layer.3.crossattention.output.dense.weight', 'bert.encoder.layer.3.crossattention.output.dense.bias', 'bert.encoder.layer.3.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.3.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.5.crossattention.self.query.weight', 'bert.encoder.layer.5.crossattention.self.query.bias', 'bert.encoder.layer.5.crossattention.self.key.weight', 'bert.encoder.layer.5.crossattention.self.key.bias', 'bert.encoder.layer.5.crossattention.self.value.weight', 'bert.encoder.layer.5.crossattention.self.value.bias', 'bert.encoder.layer.5.crossattention.output.dense.weight', 'bert.encoder.layer.5.crossattention.output.dense.bias', 'bert.encoder.layer.5.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.5.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.7.crossattention.self.query.weight', 'bert.encoder.layer.7.crossattention.self.query.bias', 'bert.encoder.layer.7.crossattention.self.key.weight', 'bert.encoder.layer.7.crossattention.self.key.bias', 'bert.encoder.layer.7.crossattention.self.value.weight', 'bert.encoder.layer.7.crossattention.self.value.bias', 'bert.encoder.layer.7.crossattention.output.dense.weight', 'bert.encoder.layer.7.crossattention.output.dense.bias', 'bert.encoder.layer.7.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.7.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.9.crossattention.self.query.weight', 'bert.encoder.layer.9.crossattention.self.query.bias', 'bert.encoder.layer.9.crossattention.self.key.weight', 'bert.encoder.layer.9.crossattention.self.key.bias', 'bert.encoder.layer.9.crossattention.self.value.weight', 'bert.encoder.layer.9.crossattention.self.value.bias', 'bert.encoder.layer.9.crossattention.output.dense.weight', 'bert.encoder.layer.9.crossattention.output.dense.bias', 'bert.encoder.layer.9.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.9.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.11.crossattention.self.query.weight', 'bert.encoder.layer.11.crossattention.self.query.bias', 'bert.encoder.layer.11.crossattention.self.key.weight', 'bert.encoder.layer.11.crossattention.self.key.bias', 'bert.encoder.layer.11.crossattention.self.value.weight', 'bert.encoder.layer.11.crossattention.self.value.bias', 'bert.encoder.layer.11.crossattention.output.dense.weight', 'bert.encoder.layer.11.crossattention.output.dense.bias', 'bert.encoder.layer.11.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.11.crossattention.output.LayerNorm.bias', 'cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import nlp\n",
    "import logging\n",
    "from datasets import load_metric\n",
    "from transformers import EncoderDecoderModel, Trainer, TrainingArguments, AutoTokenizer, set_seed\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "from tweetNormalizer import normalizeTweet\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from emoji import demojize\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "nltk_tokenizer = TweetTokenizer()\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "SEED = 42\n",
    "SAME_INOUTPUT_RATE = 0\n",
    "TWEET_COPY_TASK = 0\n",
    "\n",
    "#change 6 places\n",
    "\n",
    "#bert-base-uncased, gpt2, roberta-base, vinai/bertweet-base, google/electra-base-discriminator(only encoder)\n",
    "RUN_NAME=\"bert2bert_notebook\"\n",
    "ENCODER = \"bert-base-uncased\"\n",
    "DECODER = \"bert-base-uncased\"\n",
    "tie_ENCODER_DECODER=True\n",
    "OUTPUT_DIR=\"./models/\"+RUN_NAME+\"/2/\"\n",
    "\n",
    "RUN_NAME=\"bert2bert_notebook\"\n",
    "RUN_NAME = \"zonghaiyao tweetnorm \" + RUN_NAME\n",
    "ENTITY, PROJECT, NAME = RUN_NAME.split()\n",
    "\n",
    "batch_size = 16   # set batch size here\n",
    "encoder_length = 128\n",
    "decoder_length = 128\n",
    "\n",
    "PATH_TO_TRAIN_DATA = \"WNUT2015_dataset/train_data.json\"\n",
    "PATH_TO_VAL_DATA = \"WNUT2015_dataset/test_truth.json\"\n",
    "is_alignEmbed = True\n",
    "\n",
    "rouge = load_metric('rouge', experiment_id=9)\n",
    "bleu = load_metric('bleu', experiment_id=9)\n",
    "\n",
    "#---------------------------------------------------------------------------------------------\n",
    "# encoder tokenizer\n",
    "encoder_tokenizer = AutoTokenizer.from_pretrained(ENCODER)\n",
    "\n",
    "if ENCODER==\"bert-base-uncased\" or \"google/electra-base-discriminator\":\n",
    "    # CLS token will work as BOS token, SEP token will work as EOS token\n",
    "    encoder_tokenizer.bos_token = encoder_tokenizer.cls_token\n",
    "    encoder_tokenizer.eos_token = encoder_tokenizer.sep_token\n",
    "    \n",
    "# decoder tokenizer\n",
    "if DECODER==\"gpt2\":\n",
    "    # make sure GPT2 appends EOS in begin and end\n",
    "    def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n",
    "        outputs = [self.bos_token_id] + token_ids_0 + [self.eos_token_id]\n",
    "        return outputs\n",
    "    \n",
    "    AutoTokenizer.build_inputs_with_special_tokens = build_inputs_with_special_tokens\n",
    "    decoder_tokenizer = AutoTokenizer.from_pretrained(DECODER)\n",
    "    # set pad_token_id to unk_token_id -> be careful here as unk_token_id == eos_token_id == bos_token_id\n",
    "    decoder_tokenizer.pad_token = decoder_tokenizer.unk_token\n",
    "else:   \n",
    "    decoder_tokenizer = AutoTokenizer.from_pretrained(DECODER)\n",
    "\n",
    "if DECODER==\"bert-base-uncased\":\n",
    "    # CLS token will work as BOS token, SEP token will work as EOS token\n",
    "    decoder_tokenizer.bos_token = decoder_tokenizer.cls_token\n",
    "    decoder_tokenizer.eos_token = decoder_tokenizer.sep_token\n",
    "    \n",
    "# set EncoderDecoderModel\n",
    "model = EncoderDecoderModel.from_encoder_decoder_pretrained(ENCODER, DECODER, tie_encoder_decoder = tie_ENCODER_DECODER)\n",
    "\n",
    "# set decoding params\n",
    "model.config.decoder_start_token_id = decoder_tokenizer.bos_token_id\n",
    "model.config.eos_token_id = decoder_tokenizer.eos_token_id\n",
    "model.config.max_length = decoder_length\n",
    "model.config.min_length = 0\n",
    "model.config.no_repeat_ngram_size = 3\n",
    "model.early_stopping = True\n",
    "model.length_penalty = 2.0\n",
    "model.num_beams = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------------------------\n",
    "# load train and validation data\n",
    "train_dataset = pd.read_json(PATH_TO_TRAIN_DATA, orient=\"records\")\n",
    "val_dataset = pd.read_json(PATH_TO_VAL_DATA, orient=\"records\")\n",
    "\n",
    "#---------------------------------\n",
    "#do some normalization ourselves\n",
    "def norm(token):\n",
    "    if token.lower().startswith(\"@\"):\n",
    "        return \"username\"\n",
    "    elif token.lower().startswith('#'):\n",
    "        return \"hashtag\"\n",
    "    elif token.lower().startswith(\"http\") or token.lower().startswith(\"www\"):\n",
    "        return \"httpurl\"\n",
    "    else:\n",
    "        return token.replace(\"’\", \"'\").replace(\"…\", \"...\")\n",
    "\n",
    "def pre_pocessing_input(x):\n",
    "    result = []\n",
    "    for item in x:\n",
    "        item = norm(item)\n",
    "        #the reason for encode+decode is \"?!!\" need to be \"? ! !\"\n",
    "        input_ids = encoder_tokenizer(item).input_ids\n",
    "        item = encoder_tokenizer.decode(input_ids, skip_special_tokens=True)\n",
    "        result.append(item)\n",
    "    return result\n",
    "\n",
    "def pre_pocessing_output(x):\n",
    "    result = []\n",
    "    for item in x:\n",
    "        item = norm(item)\n",
    "        #the reason for encode+decode is \"?!!\" need to be \"? ! !\"\n",
    "        input_ids = decoder_tokenizer(item).input_ids\n",
    "        item = decoder_tokenizer.decode(input_ids, skip_special_tokens=True)\n",
    "        result.append(item)\n",
    "    return result\n",
    "\n",
    "train_dataset['input'] = train_dataset['input'].apply(pre_pocessing_input)\n",
    "train_dataset['output'] = train_dataset['output'].apply(pre_pocessing_output)\n",
    "val_dataset['input'] = val_dataset['input'].apply(pre_pocessing_input)\n",
    "val_dataset['output'] = val_dataset['output'].apply(pre_pocessing_output)\n",
    "#-----------------------------------------\n",
    "#check if it is tweet-copy task\n",
    "if TWEET_COPY_TASK == 1:\n",
    "    make_input_output_same = lambda x: x['output'].copy()\n",
    "    train_dataset['input'] = train_dataset.apply(make_input_output_same, axis=1)\n",
    "    val_dataset['input'] = val_dataset.apply(make_input_output_same, axis=1)\n",
    "#do some data augumentation\n",
    "elif SAME_INOUTPUT_RATE > 0: \n",
    "    train_dataset_no_chage_data = train_dataset.copy()\n",
    "    train_dataset_no_chage_data = train_dataset_no_chage_data.sample(frac=1, random_state=SEED)\n",
    "    origin_input_size = train_dataset_no_chage_data.shape[0]\n",
    "    train_dataset_no_chage_data = train_dataset_no_chage_data.iloc[:int(SAME_INOUTPUT_RATE * origin_input_size)]\n",
    "    make_input_output_same = lambda x: x['output'].copy()\n",
    "    train_dataset_no_chage_data['input'] = train_dataset_no_chage_data.apply(make_input_output_same, axis=1)\n",
    "    train_dataset = train_dataset.append(train_dataset_no_chage_data, ignore_index=True)\n",
    "    train_dataset = train_dataset.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "#-------------------------------------------\n",
    "#make sentence for token list\n",
    "make_sentence = lambda x : \" \".join(x).lower()\n",
    "\n",
    "train_dataset['input_sentence'] = train_dataset['input'].apply(make_sentence)\n",
    "train_dataset['output_sentence'] = train_dataset['output'].apply(make_sentence)\n",
    "val_dataset['input_sentence'] = val_dataset['input'].apply(make_sentence)\n",
    "val_dataset['output_sentence'] = val_dataset['output'].apply(make_sentence)\n",
    "\n",
    "#------------------------------------------\n",
    "#make is_labels_need_change for previous metrics\n",
    "#also calculate token_align_ids here, only for input, so use encode_tokenizer\n",
    "labels_word2id = {}\n",
    "def make_special_input_output(x):\n",
    "    assert len(x['input']) == len(x['output'])\n",
    "    \n",
    "    input_token = []\n",
    "    output_token = []\n",
    "    \n",
    "    for i in range(len(x['input'])):\n",
    "        input_token.append(x['input'][i].split())\n",
    "        output_token.append(x['output'][i].split())\n",
    "\n",
    "    #then we make is_labels_need_change\n",
    "    result = {}\n",
    "    is_labels_need_change = []\n",
    "    #align_ids for bos token is 0\n",
    "    align_ids = [0]\n",
    "    for i in range(len(input_token)):\n",
    "        if len(input_token[i]) == len(output_token[i]):\n",
    "            for j in range(len(output_token[i])):\n",
    "                #add token into labels_word2id dict\n",
    "                if output_token[i][j] not in labels_word2id.keys():\n",
    "                    labels_word2id[output_token[i][j]] = len(labels_word2id)\n",
    "                #they are the same, no need change, and align_ids + 0(keep) * ids_num\n",
    "                if output_token[i][j].lower() == input_token[i][j].lower():\n",
    "                    is_labels_need_change.append([labels_word2id[output_token[i][j]], 0])\n",
    "                    length = len(encoder_tokenizer(input_token[i][j].lower(), add_special_tokens=False).input_ids)\n",
    "                    align_ids.extend(list(np.zeros(length, dtype = np.int8)))\n",
    "                #they are diff, need change, and align_ids + 1(norm) * ids_num\n",
    "                else:\n",
    "                    is_labels_need_change.append([labels_word2id[output_token[i][j]], 1])\n",
    "                    length = len(encoder_tokenizer(input_token[i][j].lower(), add_special_tokens=False).input_ids)\n",
    "                    align_ids.extend(list(np.ones(length, dtype = np.int8)))\n",
    "        else:\n",
    "            for j in range(len(output_token[i])):\n",
    "                #add token into labels_word2id dict\n",
    "                if output_token[i][j] not in labels_word2id.keys():\n",
    "                    labels_word2id[output_token[i][j]] = len(labels_word2id)\n",
    "                #they are diff, need change\n",
    "                is_labels_need_change.append([labels_word2id[output_token[i][j]], 1])\n",
    "            for j in range(len(input_token[i])):\n",
    "                #they are diff, align_ids + 1(norm) * ids_num\n",
    "                length = len(encoder_tokenizer(input_token[i][j].lower(), add_special_tokens=False).input_ids)\n",
    "                align_ids.extend(list(np.ones(length, dtype = np.int8)))\n",
    "    \n",
    "    #align_ids for eos token is 0\n",
    "    align_ids.append(0)\n",
    "    #pad 0 to max_encoder_length\n",
    "    while len(align_ids) < encoder_length : align_ids.append(0)\n",
    "    \n",
    "    return is_labels_need_change, align_ids\n",
    "\n",
    "train_dataset[['is_labels_need_change', 'align_ids']] = train_dataset.apply(make_special_input_output, axis=1, result_type=\"expand\")\n",
    "val_dataset[['is_labels_need_change', 'align_ids']] = val_dataset.apply(make_special_input_output, axis=1, result_type=\"expand\")\n",
    "\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_dataset)\n",
    "val_dataset = Dataset.from_pandas(val_dataset)\n",
    "\n",
    "labels_id2word = {v: k for k, v in labels_word2id.items()}\n",
    "#-------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rt', 'username', ':', 'words', 'that', 'describe', 'exo', ':', 'luhan', 'is', 'pain', 'chanyeol', 'is', 'sex', 'sehun', 'is', 'porn']\n",
      "['rt', 'username', ':', 'words', 'that', 'describe', 'exo', ':', 'luhan', 'is', 'pain', 'chanyeol', 'is', 'sex', 'sehun', 'is', 'porn']\n",
      "rt username : words that describe exo : luhan is pain chanyeol is sex sehun is porn\n",
      "rt username : words that describe exo : luhan is pain chanyeol is sex sehun is porn\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[0, 0], [1, 0], [2, 0], [90, 0], [88, 0], [91, 0], [92, 0], [2, 0], [93, 0], [94, 0], [95, 0], [96, 0], [94, 0], [97, 0], [98, 0], [94, 0], [99, 0]]\n"
     ]
    }
   ],
   "source": [
    "# # for check the correctness of pre-pocessing\n",
    "# i = 9\n",
    "\n",
    "# print(train_dataset.iloc[i]['output'])\n",
    "# print(train_dataset.iloc[i]['input'])\n",
    "# print(train_dataset.iloc[i]['input_sentence'])\n",
    "# print(train_dataset.iloc[i]['output_sentence'])\n",
    "# print(train_dataset.iloc[i][\"align_ids\"])\n",
    "# print(train_dataset.iloc[i][\"is_labels_need_change\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "global batch_copy\n",
    "batch_copy = None\n",
    "\n",
    "# map data correctly\n",
    "def map_to_encoder_decoder_inputs(batch):\n",
    "    global batch_copy\n",
    "    batch_copy = batch\n",
    "    # Tokenizer will automatically set [BOS] <text> [EOS]\n",
    "    inputs = encoder_tokenizer(batch[\"input_sentence\"], padding=\"max_length\", truncation=True, max_length=encoder_length)\n",
    "    outputs = decoder_tokenizer(batch[\"output_sentence\"], padding=\"max_length\", truncation=True, max_length=decoder_length)\n",
    "\n",
    "    batch[\"input_ids\"] = inputs.input_ids\n",
    "    if is_alignEmbed:\n",
    "        batch[\"token_align_ids\"] = batch[\"align_ids\"]\n",
    "   \n",
    "    batch[\"is_labels_need_change\"] = batch[\"is_labels_need_change\"]\n",
    "    batch[\"attention_mask\"] = inputs.attention_mask\n",
    "\n",
    "    batch[\"decoder_input_ids\"] = outputs.input_ids\n",
    "    batch[\"labels\"] = outputs.input_ids.copy()\n",
    "    batch[\"decoder_attention_mask\"] = outputs.attention_mask\n",
    "    \n",
    "    \n",
    "    if DECODER==\"gpt2\":\n",
    "        # complicated list comprehension here because pad_token_id alone is not good enough to know whether label should be excluded or not\n",
    "        batch[\"labels\"] = [\n",
    "            [-100 if mask == 0 else token for mask, token in mask_and_tokens] for mask_and_tokens in [zip(masks, labels) for masks, labels in zip(batch[\"decoder_attention_mask\"], batch[\"labels\"])]\n",
    "        ]\n",
    "    else:\n",
    "        # mask loss for padding\n",
    "        batch[\"labels\"] = [\n",
    "            [-100 if token == decoder_tokenizer.pad_token_id else token for token in labels] for labels in batch[\"labels\"]\n",
    "        ]\n",
    "    \n",
    "\n",
    "    assert all([len(x) == encoder_length for x in inputs.input_ids])\n",
    "    assert all([len(x) == decoder_length for x in outputs.input_ids])\n",
    "    \n",
    "    batch_copy = batch\n",
    "    \n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec3d596a2a084a4bae06d36ace4b1774",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a24c449ee952408295be1029fe31dbc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# make train dataset ready\n",
    "train_dataset = train_dataset.map(\n",
    "    map_to_encoder_decoder_inputs, batched=True, batch_size=batch_size, remove_columns=[\"input_sentence\", \"output_sentence\"],\n",
    ")\n",
    "\n",
    "if is_alignEmbed:\n",
    "    train_dataset.set_format(\n",
    "        type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"decoder_input_ids\", \"decoder_attention_mask\", \"labels\", \"is_labels_need_change\", \"token_align_ids\"],\n",
    "    )\n",
    "else:\n",
    "    train_dataset.set_format(\n",
    "        type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"decoder_input_ids\", \"decoder_attention_mask\", \"labels\", \"is_labels_need_change\"],\n",
    "    )\n",
    "\n",
    "# same for validation dataset\n",
    "val_dataset = val_dataset.map(\n",
    "    map_to_encoder_decoder_inputs, batched=True, batch_size=batch_size, remove_columns=[\"input_sentence\", \"output_sentence\"],\n",
    ")\n",
    "\n",
    "if is_alignEmbed:\n",
    "    val_dataset.set_format(\n",
    "        type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"decoder_input_ids\", \"decoder_attention_mask\", \"labels\", \"is_labels_need_change\", \"token_align_ids\"],\n",
    "    )\n",
    "else:\n",
    "    val_dataset.set_format(\n",
    "        type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"decoder_input_ids\", \"decoder_attention_mask\", \"labels\", \"is_labels_need_change\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5e69d1f8307439883147523eb5f60cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=30.0, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d34f20b8b36d4d1384c5f25d4285e75e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=1.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/nfs/work1/llcao/zonghaiyao/tweetNorm/datasets/src/datasets/arrow_dataset.py:835: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  return torch.tensor(x, **format_kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zonghaiyao/anaconda3/envs/tweetNorm/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55d585d850ff402ebf769b0b5bdd629f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=1.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42806e1055684f989dba14f09e2244b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=1.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c4fa029ba494b4aba22ef2238668834",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=1.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed10b2b9377042699aa91ce861ccc6d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=1.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bdfe6f59c8d4645865749e44b5fb772",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=1.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "614cefa655414f4685717d805f822ef4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=1.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2e5c4a5ff82466c8414ceac89b4dd41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=1.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "867355e6cbb746bbbf99dc1206ea078e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=1.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d378f8f617ce481ea38c7cf329441248",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=1.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39743bea7dee47d9ac27d07d6833736f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=1.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdef9a3662a04416a18fb830b652e4c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=1.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bfee0caa02447ffaaca0c6978369553",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=1.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b02a9056825463885123d126af76d06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=1.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3b7c9fc045f445cbcc821027d5953fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=1.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38ac11d9e0914e55bb96ef1d4b5bfb25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=1.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa76909d4d1e46abadae1212eef3448a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=1.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4b59f3d46ab4c9b8adee074cc568ec0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=1.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23c85aeae07c41edbb2b7becfd6e9fea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=1.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14d7ab7b19bb4154bf707846dda68f72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=1.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:wandb.jupyter:Failed to query for notebook name, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable\n",
      "wandb: Currently logged in as: iesl-boxes (use `wandb login --relogin` to force relogin)\n",
      "wandb: wandb version 0.10.6 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "wandb: Tracking run with wandb version 0.10.2\n",
      "wandb: Run data is saved locally in wandb/run-20201015_173118-2mmfk6ks\n",
      "wandb: Syncing run bert2bert_notebook\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/zonghaiyao/tweetnorm\" target=\"_blank\">https://wandb.ai/zonghaiyao/tweetnorm</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/zonghaiyao/tweetnorm/runs/2mmfk6ks\" target=\"_blank\">https://wandb.ai/zonghaiyao/tweetnorm/runs/2mmfk6ks</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'loss': 9.056087493896484, 'learning_rate': 2e-05, 'epoch': 20.0, 'total_flos': 42538989772800, 'step': 20}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/nfs/work1/llcao/zonghaiyao/tweetNorm/transformers/src/transformers/trainer.py:1449: FutureWarning: The `_prediction_loop` method is deprecated and won't be called in a future version, define `prediction_loop` in your subclass.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61ae9a8aae8b4944be4298beea547d53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluation', max=1.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 102 (first `eos_token_id`) to generate sequence\n",
      "INFO:filelock:Lock 46917422570384 acquired on /home/zonghaiyao/.cache/huggingface/metrics/rouge/default/9-1-0.arrow.lock\n",
      "INFO:filelock:Lock 46917422570384 released on /home/zonghaiyao/.cache/huggingface/metrics/rouge/default/9-1-0.arrow.lock\n",
      "INFO:filelock:Lock 46917422513744 acquired on /home/zonghaiyao/.cache/huggingface/metrics/rouge/default/9-1-0.arrow.lock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:/mnt/nfs/work1/llcao/zonghaiyao/tweetNorm/datasets/src/datasets/metric.py:Removing /home/zonghaiyao/.cache/huggingface/metrics/rouge/default/9-1-0.arrow\n",
      "INFO:filelock:Lock 46917422513744 released on /home/zonghaiyao/.cache/huggingface/metrics/rouge/default/9-1-0.arrow.lock\n",
      "INFO:filelock:Lock 46917422624848 acquired on /home/zonghaiyao/.cache/huggingface/metrics/bleu/default/9-1-0.arrow.lock\n",
      "INFO:filelock:Lock 46917422624848 released on /home/zonghaiyao/.cache/huggingface/metrics/bleu/default/9-1-0.arrow.lock\n",
      "INFO:filelock:Lock 46917426162960 acquired on /home/zonghaiyao/.cache/huggingface/metrics/bleu/default/9-1-0.arrow.lock\n",
      "INFO:/mnt/nfs/work1/llcao/zonghaiyao/tweetNorm/datasets/src/datasets/metric.py:Removing /home/zonghaiyao/.cache/huggingface/metrics/bleu/default/9-1-0.arrow\n",
      "INFO:filelock:Lock 46917426162960 released on /home/zonghaiyao/.cache/huggingface/metrics/bleu/default/9-1-0.arrow.lock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 7.7961297035217285, 'eval_rouge1_precision': 0.0332, 'eval_rouge1_recall': 0.085, 'eval_rouge1_fmeasure': 0.0473, 'eval_rouge2_precision': 0.0, 'eval_rouge2_recall': 0.0, 'eval_rouge2_fmeasure': 0.0, 'eval_rouge3_precision': 0.0, 'eval_rouge3_recall': 0.0, 'eval_rouge3_fmeasure': 0.0, 'eval_rouge4_precision': 0.0, 'eval_rouge4_recall': 0.0, 'eval_rouge4_fmeasure': 0.0, 'eval_rougeL_precision': 0.0301, 'eval_rougeL_recall': 0.0753, 'eval_rougeL_fmeasure': 0.0428, 'eval_rougeLsum_precision': 0.03, 'eval_rougeLsum_recall': 0.0759, 'eval_rougeLsum_fmeasure': 0.0428, 'eval_bleu': 0.0, 'eval_norm_correct': 0.0, 'eval_norm_wrong': 29.0, 'eval_keep_correct': 2.0, 'eval_keep_wrong': 151.0, 'eval_correct_of_all_need_norm': 0.0, 'eval_correct_of_all_need_keep': 0.0131, 'eval_correct_token': 0.011, 'epoch': 20.0, 'total_flos': 42538989772800, 'step': 20}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86db7a6c9c6640b7a4b49f8d1751d70e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=1.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "754bc8a2a2aa41c3aa909fda0042eb3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=1.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e38aadced1f4ba68351e7421fca2669",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=1.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02bfb0ade47645329af6ea6839b555c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=1.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78d717ddbcb64dc389dde19c3a50c93c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=1.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d54899153c9438f8919f049d1073a7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=1.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4585d01449174b71885a8f5eebd20eef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=1.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a481907828849979f795c6718ae1d4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=1.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b23e8e0578f84a2aa75e63796996e151",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=1.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00356f1bef3248aab63ff51fd8af9db2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=1.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=30, training_loss=8.082954915364583)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#-----------------------------------------------------------------------------------\n",
    "# global record\n",
    "# record = {}\n",
    "# load metrics for validation\n",
    "# Calculate previous_metrics score of a corpus.\n",
    "def previous_metrics(pred):\n",
    "#     global record\n",
    "    pred_ids = pred.predictions\n",
    "    is_labels_need_change = pred.is_labels_need_change\n",
    "\n",
    "    def batch_convert_ids_to_tokens(sequences, **kwargs):\n",
    "        return [decoder_tokenizer.decode(seq, **kwargs) for seq in sequences]\n",
    "\n",
    "    pred_tokens = batch_convert_ids_to_tokens(pred_ids, skip_special_tokens=True)\n",
    "\n",
    "    assert len(pred_tokens) == len(is_labels_need_change)\n",
    "    norm_correct, norm_wrong, keep_correct, keep_wrong = 0.0, 0.0, 0.0, 0.0\n",
    "\n",
    "    for pred_tokens, oracle_tokens in zip(pred_tokens, is_labels_need_change):\n",
    "        pred_tokens = pred_tokens.split()\n",
    "        sent_length = len(oracle_tokens)\n",
    "\n",
    "        while len(pred_tokens) < len(oracle_tokens) : pred_tokens.append(\"<PAD>\")\n",
    "#         print(\"pred_tokens:\", pred_tokens)\n",
    "#         print(\"oracle_tokens:\", oracle_tokens)\n",
    "#         record['pred_tokens'] = pred_tokens\n",
    "#         record['oracle_tokens'] = oracle_tokens\n",
    "        \n",
    "        for i in range(sent_length):\n",
    "            pred_token = pred_tokens[i]\n",
    "            oracle_token = labels_id2word[int(oracle_tokens[i][0])]\n",
    "            token_need_change = oracle_tokens[i][1]\n",
    "            #norm-correct：需要改且改对的\n",
    "            if token_need_change == 1 and oracle_token.lower() == pred_token.lower() and oracle_token.strip():\n",
    "                norm_correct += 1\n",
    "            #norm-wrong ：需要改但没改对的\n",
    "            if token_need_change == 1 and oracle_token.lower() != pred_token.lower() and oracle_token.strip():\n",
    "                norm_wrong += 1\n",
    "            #keep-correct：不需要改且没有改的\n",
    "            if token_need_change == 0 and oracle_token.lower() == pred_token.lower() and oracle_token.strip():\n",
    "                keep_correct += 1\n",
    "            #keep-wrong ：不需要改但是改了的\n",
    "            if token_need_change == 0 and oracle_token.lower() != pred_token.lower() and oracle_token.strip():\n",
    "                keep_wrong += 1\n",
    "    \n",
    "    #所有不需要修改token的正确率\n",
    "    correct_of_all_need_keep = keep_correct / (keep_correct + keep_wrong)\n",
    "\n",
    "    results = {}\n",
    "    results[\"keep_correct\"] = keep_correct\n",
    "    results[\"keep_wrong\"] = keep_wrong\n",
    "    results[\"correct_of_all_need_keep\"] = correct_of_all_need_keep\n",
    "    \n",
    "    if TWEET_COPY_TASK == 0:\n",
    "        #所有需要修改token的正确率\n",
    "        correct_of_all_need_norm = norm_correct / (norm_correct + norm_wrong)\n",
    "        #输出正确token的正确率\n",
    "        correct_token = (norm_correct + keep_correct) / (norm_correct + norm_wrong + keep_correct + keep_wrong)\n",
    "        results[\"norm_correct\"] = norm_correct\n",
    "        results[\"norm_wrong\"] = norm_wrong\n",
    "        results[\"correct_of_all_need_norm\"] = correct_of_all_need_norm\n",
    "        results[\"correct_token\"] = correct_token\n",
    "    \n",
    "    return results\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    #Calculate previous_metrics score of a corpus\n",
    "    previous_metrics_results = previous_metrics(pred)\n",
    "    \n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "\n",
    "    # all unnecessary tokens are removed\n",
    "    pred_str = decoder_tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    labels_ids[labels_ids == -100] = decoder_tokenizer.eos_token_id\n",
    "    label_str = decoder_tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "    \n",
    "    metrics_rouge = rouge.compute(predictions=pred_str, references=label_str, rouge_types=[\"rouge1\", \"rouge2\", \"rouge3\", \"rouge4\", \"rougeL\", \"rougeLsum\"])\n",
    "\n",
    "    def batch_convert_ids_to_tokens(sequences, **kwargs):\n",
    "        return [decoder_tokenizer.convert_ids_to_tokens(seq, **kwargs) for seq in sequences]\n",
    "    \n",
    "    pred_tokens = batch_convert_ids_to_tokens(pred_ids, skip_special_tokens=True)\n",
    "    \n",
    "    def batch_convert_ids_to_tokens(sequences, **kwargs):\n",
    "        return [[decoder_tokenizer.convert_ids_to_tokens(seq, **kwargs)] for seq in sequences]\n",
    "    \n",
    "    label_tokens = batch_convert_ids_to_tokens(labels_ids, skip_special_tokens=True)\n",
    "    \n",
    "    metrics_bleu = bleu.compute(predictions=pred_tokens, references=label_tokens)\n",
    "    \n",
    "    if TWEET_COPY_TASK == 1:\n",
    "        return {\n",
    "            \"rouge1_precision\": round(metrics_rouge['rouge1'].mid.precision, 4),\n",
    "            \"rouge1_recall\": round(metrics_rouge['rouge1'].mid.recall, 4),\n",
    "            \"rouge1_fmeasure\": round(metrics_rouge['rouge1'].mid.fmeasure, 4),\n",
    "            \"rouge2_precision\": round(metrics_rouge['rouge2'].mid.precision, 4),\n",
    "            \"rouge2_recall\": round(metrics_rouge['rouge2'].mid.recall, 4),\n",
    "            \"rouge2_fmeasure\": round(metrics_rouge['rouge2'].mid.fmeasure, 4),\n",
    "            \"rouge3_precision\": round(metrics_rouge['rouge3'].mid.precision, 4),\n",
    "            \"rouge3_recall\": round(metrics_rouge['rouge3'].mid.recall, 4),\n",
    "            \"rouge3_fmeasure\": round(metrics_rouge['rouge3'].mid.fmeasure, 4),\n",
    "            \"rouge4_precision\": round(metrics_rouge['rouge4'].mid.precision, 4),\n",
    "            \"rouge4_recall\": round(metrics_rouge['rouge4'].mid.recall, 4),\n",
    "            \"rouge4_fmeasure\": round(metrics_rouge['rouge4'].mid.fmeasure, 4),\n",
    "            \"rougeL_precision\": round(metrics_rouge['rougeL'].mid.precision, 4),\n",
    "            \"rougeL_recall\": round(metrics_rouge['rougeL'].mid.recall, 4),\n",
    "            \"rougeL_fmeasure\": round(metrics_rouge['rougeL'].mid.fmeasure, 4),\n",
    "            \"rougeLsum_precision\": round(metrics_rouge['rougeLsum'].mid.precision, 4),\n",
    "            \"rougeLsum_recall\": round(metrics_rouge['rougeLsum'].mid.recall, 4),\n",
    "            \"rougeLsum_fmeasure\": round(metrics_rouge['rougeLsum'].mid.fmeasure, 4),\n",
    "            \"bleu\": round(metrics_bleu['bleu'], 4),\n",
    "            \"keep_correct\": round(previous_metrics_results['keep_correct'], 4),\n",
    "            \"keep_wrong\": round(previous_metrics_results['keep_wrong'], 4),\n",
    "            \"correct_of_all_need_keep\": round(previous_metrics_results['correct_of_all_need_keep'], 4),\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            \"rouge1_precision\": round(metrics_rouge['rouge1'].mid.precision, 4),\n",
    "            \"rouge1_recall\": round(metrics_rouge['rouge1'].mid.recall, 4),\n",
    "            \"rouge1_fmeasure\": round(metrics_rouge['rouge1'].mid.fmeasure, 4),\n",
    "            \"rouge2_precision\": round(metrics_rouge['rouge2'].mid.precision, 4),\n",
    "            \"rouge2_recall\": round(metrics_rouge['rouge2'].mid.recall, 4),\n",
    "            \"rouge2_fmeasure\": round(metrics_rouge['rouge2'].mid.fmeasure, 4),\n",
    "            \"rouge3_precision\": round(metrics_rouge['rouge3'].mid.precision, 4),\n",
    "            \"rouge3_recall\": round(metrics_rouge['rouge3'].mid.recall, 4),\n",
    "            \"rouge3_fmeasure\": round(metrics_rouge['rouge3'].mid.fmeasure, 4),\n",
    "            \"rouge4_precision\": round(metrics_rouge['rouge4'].mid.precision, 4),\n",
    "            \"rouge4_recall\": round(metrics_rouge['rouge4'].mid.recall, 4),\n",
    "            \"rouge4_fmeasure\": round(metrics_rouge['rouge4'].mid.fmeasure, 4),\n",
    "            \"rougeL_precision\": round(metrics_rouge['rougeL'].mid.precision, 4),\n",
    "            \"rougeL_recall\": round(metrics_rouge['rougeL'].mid.recall, 4),\n",
    "            \"rougeL_fmeasure\": round(metrics_rouge['rougeL'].mid.fmeasure, 4),\n",
    "            \"rougeLsum_precision\": round(metrics_rouge['rougeLsum'].mid.precision, 4),\n",
    "            \"rougeLsum_recall\": round(metrics_rouge['rougeLsum'].mid.recall, 4),\n",
    "            \"rougeLsum_fmeasure\": round(metrics_rouge['rougeLsum'].mid.fmeasure, 4),\n",
    "            \"bleu\": round(metrics_bleu['bleu'], 4),\n",
    "            \"norm_correct\": round(previous_metrics_results['norm_correct'], 4),\n",
    "            \"norm_wrong\": round(previous_metrics_results['norm_wrong'], 4),\n",
    "            \"keep_correct\": round(previous_metrics_results['keep_correct'], 4),\n",
    "            \"keep_wrong\": round(previous_metrics_results['keep_wrong'], 4),\n",
    "            \"correct_of_all_need_norm\": round(previous_metrics_results['correct_of_all_need_norm'], 4),\n",
    "            \"correct_of_all_need_keep\": round(previous_metrics_results['correct_of_all_need_keep'], 4),\n",
    "            \"correct_token\": round(previous_metrics_results['correct_token'], 4),\n",
    "        }\n",
    "        \n",
    "#----------------------------------------------------------------------------------------\n",
    "# begin train\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    predict_from_generate=True,\n",
    "    evaluate_during_training=True,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    logging_steps=20,\n",
    "    save_steps=100,\n",
    "    eval_steps=20,\n",
    "    overwrite_output_dir=True,\n",
    "    warmup_steps=50,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=30,\n",
    "\n",
    "    \n",
    "    seed=SEED,\n",
    "    fp16=True,\n",
    "    run_name=RUN_NAME,\n",
    ")\n",
    "\n",
    "\n",
    "# instantiate trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    my_trainer = True\n",
    ")\n",
    "\n",
    "\n",
    "# start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate previous_metrics score of a corpus.\n",
    "# def previous_metrics(pred):\n",
    "#     pred_ids = pred.predictions\n",
    "#     is_labels_need_change = pred.is_labels_need_change\n",
    "\n",
    "#     def batch_convert_ids_to_tokens(sequences, **kwargs):\n",
    "#         return [decoder_tokenizer.decode(seq, **kwargs) for seq in sequences]\n",
    "\n",
    "#     pred_tokens = batch_convert_ids_to_tokens(pred_ids, skip_special_tokens=True)\n",
    "\n",
    "#     assert len(pred_tokens) == len(is_labels_need_change)\n",
    "#     norm_correct, norm_wrong, keep_correct, keep_wrong = 0.0, 0.0, 0.0, 0.0\n",
    "\n",
    "#     for pred_tokens, oracle_tokens in zip(pred_tokens, is_labels_need_change):\n",
    "#         sent_length = len(oracle_tokens)\n",
    "#         while len(pred_tokens) < len(oracle_tokens) : pred_tokens.append(\"<PAD>\")\n",
    "#         for i in range(sent_length):\n",
    "#             pred_token = pred_tokens[i]\n",
    "#             oracle_token = labels_id2word[int(oracle_tokens[i][0])]\n",
    "#             token_need_change = oracle_tokens[i][1]\n",
    "#             #norm-correct：需要改且改对的\n",
    "#             if token_need_change == 1 and oracle_token.lower() == pred_token.lower() and oracle_token.strip():\n",
    "#                 norm_correct += 1\n",
    "#             #norm-wrong ：需要改但没改对的\n",
    "#             if token_need_change == 1 and oracle_token.lower() != pred_token.lower() and oracle_token.strip():\n",
    "#                 norm_wrong += 1\n",
    "#             #keep-correct：不需要改且没有改的\n",
    "#             if token_need_change == 0 and oracle_token.lower() == pred_token.lower() and oracle_token.strip():\n",
    "#                 keep_correct += 1\n",
    "#             #keep-wrong ：不需要改但是改了的\n",
    "#             if token_need_change == 0 and oracle_token.lower() != pred_token.lower() and oracle_token.strip():\n",
    "#                 keep_wrong += 1\n",
    "#     #所有需要修改token的正确率\n",
    "#     correct_of_all_need_norm = norm_correct / (norm_correct + norm_wrong)\n",
    "#     #所有不需要修改token的正确率\n",
    "#     correct_of_all_need_keep = keep_correct / (keep_correct + keep_wrong)\n",
    "#     #输出正确token的正确率\n",
    "#     correct_token = (norm_correct + keep_correct) / (norm_correct + norm_wrong + keep_correct + keep_wrong)\n",
    "\n",
    "#     results = {}\n",
    "#     results[\"norm_correct\"] = norm_correct\n",
    "#     results[\"norm_wrong\"] = norm_wrong\n",
    "#     results[\"keep_correct\"] = keep_correct\n",
    "#     results[\"keep_wrong\"] = keep_wrong\n",
    "#     results[\"correct_of_all_need_norm\"] = correct_of_all_need_norm\n",
    "#     results[\"correct_of_all_need_keep\"] = correct_of_all_need_keep\n",
    "#     results[\"correct_token\"] = correct_token\n",
    "#     return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'norm_correct': 0.0,\n",
       " 'norm_wrong': 107.0,\n",
       " 'keep_correct': 20.0,\n",
       " 'keep_wrong': 1471.0,\n",
       " 'correct_of_all_need_norm': 0.0,\n",
       " 'correct_of_all_need_keep': 0.01341381623071764,\n",
       " 'correct_token': 0.012515644555694618}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# previous_metrics(pred_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load train and validation data\n",
    "# train_df = pd.read_json(\"WNUT2015_dataset/train_data.json\", orient=\"records\")\n",
    "# val_df = pd.read_json(\"WNUT2015_dataset/test_truth.json\", orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41 41 41 41\n",
      "4 4 4 4\n"
     ]
    }
   ],
   "source": [
    "# get_len = lambda x : len(x)\n",
    "\n",
    "# train_df['input_length'] = train_df['input'].apply(get_len)\n",
    "# train_df['output_length'] = train_df['output'].apply(get_len)\n",
    "# val_df['input_length'] = val_df['input'].apply(get_len)\n",
    "# val_df['output_length'] = val_df['output'].apply(get_len)\n",
    "\n",
    "# print(max(train_df['input_length']), max(train_df['output_length']), max(val_df['input_length']), max(val_df['output_length']))\n",
    "# print(min(train_df['input_length']), min(train_df['output_length']), min(val_df['input_length']), min(val_df['output_length']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
