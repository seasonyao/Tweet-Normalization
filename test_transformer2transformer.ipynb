{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import nlp\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer, EncoderDecoderModel\n",
    "tokenizer = BertTokenizer.from_pretrained(\"patrickvonplaten/bert2bert-cnn_dailymail-fp16\")\n",
    "model = EncoderDecoderModel.from_pretrained(\"patrickvonplaten/bert2bert-cnn_dailymail-fp16\")\n",
    "model.to(\"cuda\")\n",
    "test_dataset = nlp.load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"test\")\n",
    "# test_dataset = load_dataset('cnn_dailymail', \"3.0.0\", split='test', ignore_verifications=True)\n",
    "\n",
    "batch_size = 128\n",
    "# map data correctly\n",
    "def generate_summary(batch):\n",
    "    # Tokenizer will automatically set [BOS] <text> [EOS]\n",
    "    # cut off at BERT max length 512\n",
    "    inputs = tokenizer(batch[\"article\"], padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    input_ids = inputs.input_ids.to(\"cuda\")\n",
    "    attention_mask = inputs.attention_mask.to(\"cuda\")\n",
    "    outputs = model.generate(input_ids, attention_mask=attention_mask)\n",
    "    # all special tokens including will be removed\n",
    "    output_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    batch[\"pred\"] = output_str\n",
    "    return batch\n",
    "results = test_dataset.map(generate_summary, batched=True, batch_size=batch_size, remove_columns=[\"article\"])\n",
    "# load rouge for validation\n",
    "rouge = nlp.load_metric(\"rouge\")\n",
    "pred_str = results[\"pred\"]\n",
    "label_str = results[\"highlights\"]\n",
    "rouge_output = rouge.compute(predictions=pred_str, references=label_str, rouge_types=[\"rouge2\"])[\"rouge2\"].mid\n",
    "print(rouge_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# error case analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EncoderDecoderConfig, EncoderDecoderModel, AutoTokenizer\n",
    "\n",
    "encoder_decoder_config = EncoderDecoderConfig.from_pretrained('./models/bert2bert_share/5/checkpoint-9000')\n",
    "bert2bert_model = EncoderDecoderModel.from_pretrained('./models/bert2bert_share/5/checkpoint-9000', config=encoder_decoder_config)\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tweetNormalizer import normalizeTweet\n",
    "# from nltk.tokenize import TweetTokenizer\n",
    "# nltk_tknzr = TweetTokenizer()\n",
    "\n",
    "# load train and validation data\n",
    "train_dataset = pd.read_json(\"WNUT2015_dataset/train_data.json\", orient=\"records\")\n",
    "val_dataset = pd.read_json(\"WNUT2015_dataset/test_truth.json\", orient=\"records\")\n",
    "\n",
    "#make_sentence = lambda x : \" \".join(x)\n",
    "make_sentence = lambda x : normalizeTweet(\" \".join(x))\n",
    "\n",
    "train_dataset['input_sentence'] = train_dataset['input'].apply(make_sentence)\n",
    "train_dataset['output_sentence'] = train_dataset['output'].apply(make_sentence)\n",
    "val_dataset['input_sentence'] = val_dataset['input'].apply(make_sentence)\n",
    "val_dataset['output_sentence'] = val_dataset['output'].apply(make_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 102 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet:  USERNAME WE cannot DOIN A FUCKEN MOVIE , it's so bad . lol. gonna to be better HTTPURL\n",
      "model:  ['user', '##name', 'we', 'cannot', 'do', 'a', 'fucking', 'thing', ',', 'it', \"'\", 's', 'so', 'bad', '.', 'laughing', 'out', 'loud', '.', 'gonna', 'be', 'better', 'http', '##ur', '##l']\n",
      "model:  username we cannot do a fucking thing , it's so bad . laughing out loud . gonna be better httpurl\n"
     ]
    }
   ],
   "source": [
    "tweet_text = \"USERNAME WE cannot DOIN A FUCKEN MOVIE , it's so bad . lol. gonna to be better HTTPURL\"\n",
    "#tweet_text = train_dataset['input_sentence'].iloc[2]\n",
    "\n",
    "input_ids = bert_tokenizer(tweet_text, return_tensors=\"pt\").input_ids\n",
    "output_ids = bert2bert_model.generate(input_ids)\n",
    "\n",
    "print(\"tweet: \", tweet_text)\n",
    "print(\"model: \", bert_tokenizer.convert_ids_to_tokens(output_ids[0], skip_special_tokens=True))\n",
    "#print(\"model: \", \" \".join(nltk_tknzr.tokenize(bert_tokenizer.decode(output_ids[0], skip_special_tokens=True))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 102 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet:  USERNAME WE cannot DOIN A FUCKEN MOVIE , it's so bad . lol. gonna to be better HTTPURL\n",
      "model:  username we cannot do a fucking thing , it's so bad . laughing out loud . gonna be better httpurl\n",
      "model:  username we cannot do a fucking thing , it's so bad . laughing out loud . gonna be better httpurl\n"
     ]
    }
   ],
   "source": [
    "tweet_text = \"USERNAME WE cannot DOIN A FUCKEN MOVIE , it's so bad . lol. gonna to be better HTTPURL\"\n",
    "#tweet_text = train_dataset['input_sentence'].iloc[2]\n",
    "\n",
    "input_ids = bert_tokenizer(tweet_text, return_tensors=\"pt\").input_ids\n",
    "output_ids = bert2bert_model.generate(input_ids)\n",
    "\n",
    "print(\"tweet: \", tweet_text)\n",
    "print(\"model: \", bert_tokenizer.decode(output_ids[0], skip_special_tokens=True))\n",
    "#print(\"model: \", \" \".join(nltk_tknzr.tokenize(bert_tokenizer.decode(output_ids[0], skip_special_tokens=True))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 102 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 102 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet:  username yeh but still that's wild lol\n",
      "model_orig:  username yeah but still that's wild laughing out loud\n",
      "label:  username yeah but still that's wild laughing out loud\n",
      "model_nltk:  username yeah but still that's wild laughing out loud\n",
      "label_nltk:  username yeah but still that's wild laughing out loud\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 102 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet:  Dick in Janice , Im poppin xanax and speakin spanish .\n",
      "model_orig:  dick in janice , i'm poping xanax and speaking loud .\n",
      "label:  dick in janice , i'm popping xanax and speaking spanish .\n",
      "model_nltk:  dick in janice , i'm poping xanax and speaking loud .\n",
      "label_nltk:  dick in janice , i'm popping xanax and speaking spanish .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 102 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet:  UCSB I fear the next rampage will b cuz media told us everything hashtag ever diid . to get ratings . hashtag hashtag hashtag\n",
      "model_orig:  ucsb i fear the next thing will be because we told us everything hashtag ever . to get power . to getsads . hashtag hashtag\n",
      "label:  ucsb i fear the next rampage will because media told us everything hashtag ever did . to get ratings . hashtag hashtag hashtag\n",
      "model_nltk:  ucsb i fear the next thing will be because we told us everything hashtag ever . to get power . to getsads . hashtag hashtag\n",
      "label_nltk:  ucsb i fear the next rampage will because media told us everything hashtag ever did . to get ratings . hashtag hashtag hashtag\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 102 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet:  RT username : username username username not even gays are gonna look to u LMAO\n",
      "model_orig:  rt username : username username usersname not even gays are gonna look to you laughing my ass off\n",
      "label:  rt username : username username username not even gays are gonna look to you laughing my ass off\n",
      "model_nltk:  rt username : username username usersname not even gays are gonna look to you laughing my ass off\n",
      "label_nltk:  rt username : username username username not even gays are gonna look to you laughing my ass off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 102 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet:  username username username card in 5 mins is with wh but this was the game so far bk lol but how to find who the ref is\n",
      "model_orig:  username username card usernamecard in 5 minutes is with this one but this was the game so far out laughing out loud but how the fuck is i\n",
      "label:  username username username card in 5 minutes is with wh but this was the game so far bk laughing out loud but how to find who the ref is\n",
      "model_nltk:  username username card usernamecard in 5 minutes is with this one but this was the game so far out laughing out loud but how the fuck is i\n",
      "label_nltk:  username username username card in 5 minutes is with wh but this was the game so far bk laughing out loud but how to find who the ref is\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 102 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet:  username haha oh okk ! Yeaah , they've been dating for a while now ! ! They seem crazy cute ! ! !\n",
      "model_orig:  username haha oh okk ! yeaah , they've been dating for a while now ! ! they're crazy ! ! !\n",
      "label:  username haha oh okay ! yeah , they've been dating for a while now ! ! they seem crazy cute ! ! !\n",
      "model_nltk:  username haha oh okk ! yeaah , they've been dating for a while now ! ! they're crazy ! ! !\n",
      "label_nltk:  username haha oh okay ! yeah , they've been dating for a while now ! ! they seem crazy cute ! ! !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 102 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet:  Samsung working on Oculus Rift-like VR headsets for Galaxy devices : Report - Firstpost httpurl\n",
      "model_orig:  galaxy working on oculus rift - like vrs for galaxy galaxy device : report - first line httpurl\n",
      "label:  samsung working on oculus rift-like vr headsets for galaxy devices : report - firstpost httpurl\n",
      "model_nltk:  galaxy working on oculus rift - like vrs for galaxy galaxy device : report - first line httpurl\n",
      "label_nltk:  samsung working on oculus rift-like vr headsets for galaxy devices : report - firstpost httpurl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 102 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet:  RT username : best snapchat I've seen all day lmfao httpurl\n",
      "model_orig:  rt username : best snapchat i've seen all day laughing my fucking ass off httpurl\n",
      "label:  rt username : best snapchat i've seen all day laughing my fucking ass off httpurl\n",
      "model_nltk:  rt username : best snapchat i've seen all day laughing my fucking ass off httpurl\n",
      "label_nltk:  rt username : best snapchat i've seen all day laughing my fucking ass off httpurl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 102 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet:  Legends indeed RT username : Feela sistah Spoken Word Collective ... hashtag httpurl\n",
      "model_orig:  dreams indeed rt username : feela sistah speak word word . . . hashtag httpurl\n",
      "label:  legends indeed rt username : feels sistah spoken word collective ... hashtag httpurl\n",
      "model_nltk:  dreams indeed rt username : feela sistah speak word word . . . hashtag httpurl\n",
      "label_nltk:  legends indeed rt username : feels sistah spoken word collective ... hashtag httpurl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 102 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet:  ThisNewsSoccer : Adam Lallana Pilih Tottenham Hotspur Ketimbang Liverpool httpurl hashtag\n",
      "model_orig:  thisnewssoccer : adam lalilah tottenham fc tottenham fc kembambang liverpool httpurl hashtag\n",
      "label:  this news soccer : adam lallana pilih tottenham hotspur ketimbang liverpool httpurl hashtag\n",
      "model_nltk:  thisnewssoccer : adam lalilah tottenham fc tottenham fc kembambang liverpool httpurl hashtag\n",
      "label_nltk:  this news soccer : adam lallana pilih tottenham hotspur ketimbang liverpool httpurl hashtag\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 102 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet:  Enter to hashtag a prize pack of LINDOR truffles and LINDOR sticks ( ARCV $ 52 ) from Lindt & username ! hashtag httpurl\n",
      "model_orig:  entry to hashtag a prize pack of lindy tromoux and lindy sticks ( arc ) from 50s & username from username ! hashtag httpurl\n",
      "label:  enter to hashtag a prize pack of lindor truffles and lindor sticks ( arcv $ 52 ) from lindt & username ! hashtag httpurl\n",
      "model_nltk:  entry to hashtag a prize pack of lindy tromoux and lindy sticks ( arc ) from 50s & username from username ! hashtag httpurl\n",
      "label_nltk:  enter to hashtag a prize pack of lindor truffles and lindor sticks ( arcv $ 52 ) from lindt & username ! hashtag httpurl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 102 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet:  httpurl hashtag \" ... hashtag is the result of an aggressive mind . \" . Pema Chodron\n",
      "model_orig:  httpurl hashtag \" . . . hashtag is the result of an attack . . \" pema . .\n",
      "label:  httpurl hashtag \" ... hashtag is the result of an aggressive mind . \" . pema chodron\n",
      "model_nltk:  httpurl hashtag \" . . . hashtag is the result of an attack . . \" pema . .\n",
      "label_nltk:  httpurl hashtag \" ... hashtag is the result of an aggressive mind . \" . pema chodron\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 102 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet:  RT username : WE R DOIN A FUCKEN MOVIE WITH LIONSGATE HOW DID WE EVEN GET HERE FROM BARKING ON A TRAIN BAHAHAHAHA\n",
      "model_orig:  rt username : we are doing a fucker movie with liongate how could we get here from a train on a trainhahaha\n",
      "label:  rt username : we are doing a fucking movie with lionsgate how did we even get here from barking on a train bahahahaha\n",
      "model_nltk:  rt username : we are doing a fucker movie with liongate how could we get here from a train on a trainhahaha\n",
      "label_nltk:  rt username : we are doing a fucking movie with lionsgate how did we even get here from barking on a train bahahahaha\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 102 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet:  Lol . Don't act cute here pls\n",
      "model_orig:  laughing out loud . don't act cute here please\n",
      "label:  laughing out loud . don't act cute here please\n",
      "model_nltk:  laughing out loud . don't act cute here please\n",
      "label_nltk:  laughing out loud . don't act cute here please\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 102 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet:  RT username : Idek why I bother ... Smh\n",
      "model_orig:  rt username : i don't know why i bothered . . . shaking my head\n",
      "label:  rt username : i don't even know why i bother ... shaking my head\n",
      "model_nltk:  rt username : i don't know why i bothered . . . shaking my head\n",
      "label_nltk:  rt username : i don't even know why i bother ... shaking my head\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 102 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet:  RT username : [ HQ ] 140526 Xiumin , Luhan username MBC Idol Futsal Championship ( cr . shade of the bloom ) httpurl\n",
      "model_orig:  rt username : [ hq ] 140526 xiumin , luhan username mbc idol futsal championship ( cr . shade of the bloomer ) httpurl\n",
      "label:  rt username : [ hq ] 140526 xiumin , luhan username mbc idol futsal championship ( cr . shade of the bloom ) httpurl\n",
      "model_nltk:  rt username : [ hq ] 140526 xiumin , luhan username mbc idol futsal championship ( cr . shade of the bloomer ) httpurl\n",
      "label_nltk:  rt username : [ hq ] 140526 xiumin , luhan username mbc idol futsal championship ( cr . shade of the bloom ) httpurl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 102 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet:  username Ayy skiperooo wassup cuz what u mean by Marlton ?\n",
      "model_orig:  username ayy skips was so what you know about what you mean ?\n",
      "label:  username ayy skiperooo what's up because what you mean by marlton ?\n",
      "model_nltk:  username ayy skips was so what you know about what you mean ?\n",
      "label_nltk:  username ayy skiperooo what's up because what you mean by marlton ?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 102 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet:  RT username : Sean , Darragh and I , in croke park ! Unbelievable weekend ! httpurl\n",
      "model_orig:  rt username : sean , darragh and i , in croke park ! unbelievable weekend ! httpurl\n",
      "label:  rt username : sean , darragh and i , in croke park ! unbelievable weekend ! httpurl\n",
      "model_nltk:  rt username : sean , darragh and i , in croke park ! unbelievable weekend ! httpurl\n",
      "label_nltk:  rt username : sean , darragh and i , in croke park ! unbelievable weekend ! httpurl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 102 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet:  username umm I thought that convo was pretty funny soooooo\n",
      "model_orig:  username umm i thought that convo was pretty funny so damn\n",
      "label:  username umm i thought that conversation was pretty funny so\n",
      "model_nltk:  username umm i thought that convo was pretty funny so damn\n",
      "label_nltk:  username umm i thought that conversation was pretty funny so\n",
      "tweet:  Stop be so fucken gay you three aint nobody got time for that username username username\n",
      "model_orig:  stop be so fucking gay you three people ain't nobody got that username username\n",
      "label:  stop be so fucking gay you three ain't nobody got time for that username username username\n",
      "model_nltk:  stop be so fucking gay you three people ain't nobody got that username username\n",
      "label_nltk:  stop be so fucking gay you three ain't nobody got time for that username username username\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 20):\n",
    "#     print(\"example\"+str(i))\n",
    "    tweet_text = val_dataset.iloc[i]['input_sentence']\n",
    "    normal_english = val_dataset.iloc[i]['output_sentence'].lower()\n",
    "\n",
    "    input_ids = bert_tokenizer(tweet_text, return_tensors=\"pt\").input_ids\n",
    "    output_ids = bert2bert_model.generate(input_ids)\n",
    "\n",
    "    print(\"tweet: \", tweet_text)\n",
    "    print(\"model_orig: \", bert_tokenizer.decode(output_ids[0], skip_special_tokens=True))\n",
    "    print(\"label: \", normal_english)\n",
    "    print(\"model_nltk: \", \" \".join(nltk_tknzr.tokenize(bert_tokenizer.decode(output_ids[0], skip_special_tokens=True))))\n",
    "    print(\"label_nltk: \", \" \".join(nltk_tknzr.tokenize(normal_english)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
